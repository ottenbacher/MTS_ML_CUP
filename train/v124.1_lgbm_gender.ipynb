{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec81f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "from scipy.sparse import csr_matrix, coo_matrix, vstack, load_npz\n",
    "import implicit\n",
    "import bisect\n",
    "import sklearn.metrics as m\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "#import vaex\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73570893",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_PATH = 'context_data'\n",
    "SPLIT_SEED = 42\n",
    "DATA_FILE = 'competition_data_final_pqt'\n",
    "TARGET_FILE = 'public_train.pqt'\n",
    "SUBMISSION_FILE = 'submit_2.pqt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151dd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_submit = pq.read_table(f'../{LOCAL_DATA_PATH}/{SUBMISSION_FILE}').to_pandas()\n",
    "tgt = pq.read_table(f'../{LOCAL_DATA_PATH}/{TARGET_FILE}').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5881cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.DataFrame(index=tgt['is_male'][(tgt.is_male == '0') | (tgt.is_male == '1')].index)\n",
    "y_true = tgt['is_male'][(tgt.is_male == '0') | (tgt.is_male == '1')].values.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3d21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_feather('../utils/v121_gender_train.feather', index_col='user_id')\n",
    "test_df = pd.read_feather('../utils/v121_gender_test.feather', index_col='user_id')\n",
    "feat_df = pd.read_csv('../utils/feat_gen_df3.csv', index_col='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3771df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['os'] = train_df['os'].map({'iOS': 0, 'Android': 1})\n",
    "test_df['os'] = test_df['os'].map({'iOS': 0, 'Android': 1})\n",
    "\n",
    "cat_df = feat_df[['region_name', 'city_name', 'company', 'model']]\n",
    "cat_feat = np.stack([cat_df[col].astype('category').cat.codes.values for col in cat_df]).T\n",
    "train_df[['region_name', 'city_name', 'company', 'model']] = cat_feat[train_df.index]\n",
    "test_df[['region_name', 'city_name', 'company', 'model']] = cat_feat[test_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9613e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a17f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______fold 1______\n",
      "[LightGBM] [Info] Number of positive: 108265, number of negative: 103195\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.992672\n",
      "[LightGBM] [Info] Total Bins 121355\n",
      "[LightGBM] [Info] Number of data points in the train set: 211460, number of used features: 2524\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 355 dense feature groups (71.79 MB) transferred to GPU in 0.023215 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511988 -> initscore=0.047962\n",
      "[LightGBM] [Info] Start training from score 0.047962\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[1]\tvalid_0's binary_logloss: 0.679768\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[2]\tvalid_0's binary_logloss: 0.669632\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[3]\tvalid_0's binary_logloss: 0.662372\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[4]\tvalid_0's binary_logloss: 0.655094\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[5]\tvalid_0's binary_logloss: 0.649434\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[6]\tvalid_0's binary_logloss: 0.644028\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[7]\tvalid_0's binary_logloss: 0.638783\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[8]\tvalid_0's binary_logloss: 0.634168\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[9]\tvalid_0's binary_logloss: 0.630008\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[10]\tvalid_0's binary_logloss: 0.631302\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[11]\tvalid_0's binary_logloss: 0.627625\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[12]\tvalid_0's binary_logloss: 0.623897\n",
      "______fold 2______\n",
      "[LightGBM] [Info] Number of positive: 108265, number of negative: 103196\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.992668\n",
      "[LightGBM] [Info] Total Bins 121558\n",
      "[LightGBM] [Info] Number of data points in the train set: 211461, number of used features: 2524\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 355 dense feature groups (71.79 MB) transferred to GPU in 0.024470 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511986 -> initscore=0.047952\n",
      "[LightGBM] [Info] Start training from score 0.047952\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[1]\tvalid_0's binary_logloss: 0.679558\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[2]\tvalid_0's binary_logloss: 0.669568\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[3]\tvalid_0's binary_logloss: 0.66217\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[4]\tvalid_0's binary_logloss: 0.65479\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[5]\tvalid_0's binary_logloss: 0.649134\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[6]\tvalid_0's binary_logloss: 0.643737\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[7]\tvalid_0's binary_logloss: 0.638537\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[8]\tvalid_0's binary_logloss: 0.633464\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[9]\tvalid_0's binary_logloss: 0.629357\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[10]\tvalid_0's binary_logloss: 0.630738\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[11]\tvalid_0's binary_logloss: 0.626698\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[12]\tvalid_0's binary_logloss: 0.622959\n",
      "______fold 3______\n",
      "[LightGBM] [Info] Number of positive: 108266, number of negative: 103195\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.992671\n",
      "[LightGBM] [Info] Total Bins 121515\n",
      "[LightGBM] [Info] Number of data points in the train set: 211461, number of used features: 2524\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 355 dense feature groups (71.79 MB) transferred to GPU in 0.022622 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511990 -> initscore=0.047971\n",
      "[LightGBM] [Info] Start training from score 0.047971\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[1]\tvalid_0's binary_logloss: 0.679894\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[2]\tvalid_0's binary_logloss: 0.669862\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[3]\tvalid_0's binary_logloss: 0.662372\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[4]\tvalid_0's binary_logloss: 0.655114\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[5]\tvalid_0's binary_logloss: 0.649627\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[6]\tvalid_0's binary_logloss: 0.644111\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[7]\tvalid_0's binary_logloss: 0.639045\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[8]\tvalid_0's binary_logloss: 0.63405\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[9]\tvalid_0's binary_logloss: 0.630066\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[10]\tvalid_0's binary_logloss: 0.631319\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[11]\tvalid_0's binary_logloss: 0.627376\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[12]\tvalid_0's binary_logloss: 0.623705\n",
      "______fold 4______\n",
      "[LightGBM] [Info] Number of positive: 108266, number of negative: 103195\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.992684\n",
      "[LightGBM] [Info] Total Bins 121447\n",
      "[LightGBM] [Info] Number of data points in the train set: 211461, number of used features: 2524\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 355 dense feature groups (71.79 MB) transferred to GPU in 0.029371 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511990 -> initscore=0.047971\n",
      "[LightGBM] [Info] Start training from score 0.047971\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[1]\tvalid_0's binary_logloss: 0.679655\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[2]\tvalid_0's binary_logloss: 0.669377\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[3]\tvalid_0's binary_logloss: 0.661915\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[4]\tvalid_0's binary_logloss: 0.65459\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[5]\tvalid_0's binary_logloss: 0.648611\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[6]\tvalid_0's binary_logloss: 0.643064\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[7]\tvalid_0's binary_logloss: 0.637925\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[8]\tvalid_0's binary_logloss: 0.632965\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[9]\tvalid_0's binary_logloss: 0.628994\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[10]\tvalid_0's binary_logloss: 0.630241\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[11]\tvalid_0's binary_logloss: 0.626208\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[12]\tvalid_0's binary_logloss: 0.622357\n",
      "______fold 5______\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 108266, number of negative: 103195\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.992655\n",
      "[LightGBM] [Info] Total Bins 121522\n",
      "[LightGBM] [Info] Number of data points in the train set: 211461, number of used features: 2524\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 355 dense feature groups (71.79 MB) transferred to GPU in 0.025610 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511990 -> initscore=0.047971\n",
      "[LightGBM] [Info] Start training from score 0.047971\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[1]\tvalid_0's binary_logloss: 0.67983\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[2]\tvalid_0's binary_logloss: 0.669719\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[3]\tvalid_0's binary_logloss: 0.662229\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[4]\tvalid_0's binary_logloss: 0.655101\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[5]\tvalid_0's binary_logloss: 0.64913\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[6]\tvalid_0's binary_logloss: 0.643677\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[7]\tvalid_0's binary_logloss: 0.638433\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[8]\tvalid_0's binary_logloss: 0.633479\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[9]\tvalid_0's binary_logloss: 0.629352\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[10]\tvalid_0's binary_logloss: 0.630704\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[11]\tvalid_0's binary_logloss: 0.626498\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 4 and depth = 2\n",
      "[12]\tvalid_0's binary_logloss: 0.622712\n",
      "CPU times: total: 4min 54s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = train_df\n",
    "y = y_true\n",
    "\n",
    "models_0 = []\n",
    "params = {'boosting_type': 'dart',\n",
    "          'drop_rate': 0.1,\n",
    "          'max_drop': 50,\n",
    "          'skip_drop': 0.5,\n",
    "          'objective': 'binary',\n",
    "          'num_iterations': 12000,\n",
    "          'max_depth': 2,\n",
    "          'min_data_in_leaf': 20,\n",
    "          'bagging_fraction': 1.0,\n",
    "          'feature_fraction': 1.0,\n",
    "          'feature_fraction_bynode': 1.0,\n",
    "          'lambda_l1': 0.0,\n",
    "          'lambda_l2': 0.5,\n",
    "          'learning_rate': 0.2,\n",
    "          'num_leaves': 31,\n",
    "          'device_type': \"gpu\",\n",
    "          'num_threads': 12,\n",
    "          'early_stopping_rounds': 500,\n",
    "          'verbose': 200, # output to stdout info about training process every 200 iterations\n",
    "          'seed': 722\n",
    "         }\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for n, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f'______fold {n+1}______')\n",
    "    lgb_train = lgb.Dataset(X.iloc[train_idx], y[train_idx])\n",
    "    lgb_eval = lgb.Dataset(X.iloc[test_idx], y[test_idx], reference=lgb_train)\n",
    "    clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=lgb_eval\n",
    "                )\n",
    "    models_0.append(clf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c54a62a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48258722, 0.62648725, 0.48258722, ..., 0.64951426, 0.79862766,\n",
       "       0.53881244])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X.iloc[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b97ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtrain = lgb.Dataset(train_df, label=y_true)\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    params = {'boosting_type': 'dart',\n",
    "              'drop_rate': trial.suggest_float('drop_rate', 0.03, 0.3),\n",
    "              'max_drop': trial.suggest_int('max_drop', 5, 100),\n",
    "              'skip_drop': trial.suggest_float('skip_drop', 0.2, 0.8),\n",
    "              'objective': 'binary',\n",
    "              'num_iterations': 10000,\n",
    "              'max_depth': trial.suggest_int('max_depth', 2, 5),\n",
    "              'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "              'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "              'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n",
    "              'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "              'feature_fraction_bynode': trial.suggest_uniform('feature_fraction_bynode', 0.1, 1.0),\n",
    "              'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "              'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "              'learning_rate': trial.suggest_loguniform('learning_rate', 0.02, 0.1),\n",
    "              'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "              'device_type': \"gpu\",\n",
    "              'num_threads': 12,\n",
    "              'verbose': -1, # output to stdout info about training process every 200 iterations\n",
    "              'seed': 722\n",
    "         }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    cv_scores = np.empty(5)\n",
    "    for n, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        lgb_train = lgb.Dataset(X.iloc[train_idx], y[train_idx])\n",
    "        lgb_eval = lgb.Dataset(X.iloc[test_idx], y[test_idx], reference=lgb_train)\n",
    "        clf = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    callbacks=[\n",
    "                    LightGBMPruningCallback(trial, \"binary_logloss\")\n",
    "            ]\n",
    "                    )\n",
    "        preds = clf.predict(X.iloc[test_idx])\n",
    "        cv_scores[n] = m.log_loss(y[test_idx], preds)\n",
    "    \n",
    "\n",
    "    # Return metric of interest\n",
    "    return np.mean(cv_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56ff39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 20:17:18,756]\u001b[0m A new study created in memory with name: no-name-865fbc1e-0564-4fae-a75f-e9692aec9de3\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 21:01:32,195]\u001b[0m Trial 0 finished with value: 0.47197195530033087 and parameters: {'drop_rate': 0.23222124808984007, 'max_drop': 57, 'skip_drop': 0.33076957815092567, 'max_depth': 2, 'min_child_samples': 71, 'bagging_fraction': 0.302425202243268, 'bagging_freq': 11, 'feature_fraction': 0.25239724278280634, 'feature_fraction_bynode': 0.5299935456013781, 'lambda_l1': 0.07174265127061853, 'lambda_l2': 3.1819444005716135e-06, 'learning_rate': 0.05426499478705572, 'num_leaves': 20}. Best is trial 0 with value: 0.47197195530033087.\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 21:46:53,443]\u001b[0m Trial 1 finished with value: 0.4522203071278927 and parameters: {'drop_rate': 0.0741641090884101, 'max_drop': 6, 'skip_drop': 0.37946362239344056, 'max_depth': 5, 'min_child_samples': 23, 'bagging_fraction': 0.7118530880323163, 'bagging_freq': 7, 'feature_fraction': 0.8470271524672922, 'feature_fraction_bynode': 0.720398920107027, 'lambda_l1': 9.51177706343935e-06, 'lambda_l2': 0.00015462464588287303, 'learning_rate': 0.02018613537746642, 'num_leaves': 458}. Best is trial 1 with value: 0.4522203071278927.\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 22:20:12,343]\u001b[0m Trial 2 finished with value: 0.44579015839323344 and parameters: {'drop_rate': 0.0926445751312792, 'max_drop': 11, 'skip_drop': 0.7049137093837174, 'max_depth': 4, 'min_child_samples': 47, 'bagging_fraction': 0.2547989334157241, 'bagging_freq': 0, 'feature_fraction': 0.8001827075267284, 'feature_fraction_bynode': 0.5644033488154744, 'lambda_l1': 0.00825668326451917, 'lambda_l2': 0.004410816884611285, 'learning_rate': 0.03146819732392239, 'num_leaves': 305}. Best is trial 2 with value: 0.44579015839323344.\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 23:03:05,035]\u001b[0m Trial 3 finished with value: 0.44456589154312576 and parameters: {'drop_rate': 0.21216392056868447, 'max_drop': 64, 'skip_drop': 0.7492372614688305, 'max_depth': 3, 'min_child_samples': 23, 'bagging_fraction': 0.869444523907701, 'bagging_freq': 3, 'feature_fraction': 0.28010881378789204, 'feature_fraction_bynode': 0.3758066794086461, 'lambda_l1': 0.0008895787571355126, 'lambda_l2': 5.821650276387074e-05, 'learning_rate': 0.06515974756836698, 'num_leaves': 245}. Best is trial 3 with value: 0.44456589154312576.\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 23:33:03,793]\u001b[0m Trial 4 finished with value: 0.476878990619862 and parameters: {'drop_rate': 0.2559245213215141, 'max_drop': 57, 'skip_drop': 0.5778783994850036, 'max_depth': 2, 'min_child_samples': 35, 'bagging_fraction': 0.7834752297403704, 'bagging_freq': 1, 'feature_fraction': 0.8367877565797586, 'feature_fraction_bynode': 0.4233260750157931, 'lambda_l1': 3.387169255578294e-08, 'lambda_l2': 1.191160424263649e-05, 'learning_rate': 0.02353627637869627, 'num_leaves': 252}. Best is trial 3 with value: 0.44456589154312576.\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 23:33:17,334]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-19 23:33:33,924]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 96.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 00:04:25,152]\u001b[0m Trial 7 finished with value: 0.444580666333272 and parameters: {'drop_rate': 0.2957711215816363, 'max_drop': 24, 'skip_drop': 0.6410090365940639, 'max_depth': 3, 'min_child_samples': 32, 'bagging_fraction': 0.7128081939168145, 'bagging_freq': 2, 'feature_fraction': 0.9951398487637926, 'feature_fraction_bynode': 0.999219419966335, 'lambda_l1': 0.0015278998849324505, 'lambda_l2': 4.944234643739452e-08, 'learning_rate': 0.06669665793771007, 'num_leaves': 92}. Best is trial 3 with value: 0.44456589154312576.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 01:09:23,708]\u001b[0m Trial 8 finished with value: 0.4434747376662068 and parameters: {'drop_rate': 0.08012291391656842, 'max_drop': 87, 'skip_drop': 0.6903539138152903, 'max_depth': 4, 'min_child_samples': 9, 'bagging_fraction': 0.9490979438210462, 'bagging_freq': 0, 'feature_fraction': 0.7586920499105267, 'feature_fraction_bynode': 0.4388830269362317, 'lambda_l1': 0.0014359161989450455, 'lambda_l2': 1.2765262381641739e-08, 'learning_rate': 0.06978031930504904, 'num_leaves': 440}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 01:48:26,044]\u001b[0m Trial 9 finished with value: 0.4441158047333039 and parameters: {'drop_rate': 0.16058928750048893, 'max_drop': 16, 'skip_drop': 0.5891391311001319, 'max_depth': 3, 'min_child_samples': 24, 'bagging_fraction': 0.9537827199320618, 'bagging_freq': 8, 'feature_fraction': 0.5729614007323482, 'feature_fraction_bynode': 0.4025796496206897, 'lambda_l1': 3.601195680187364e-05, 'lambda_l2': 7.519170938281902e-06, 'learning_rate': 0.09196925402823418, 'num_leaves': 126}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 01:50:54,459]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 2184.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 01:51:09,939]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 37.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 03:22:21,741]\u001b[0m Trial 12 finished with value: 0.4441309242194462 and parameters: {'drop_rate': 0.17713194217700906, 'max_drop': 80, 'skip_drop': 0.5812282981332823, 'max_depth': 4, 'min_child_samples': 1, 'bagging_fraction': 0.9829900217739592, 'bagging_freq': 5, 'feature_fraction': 0.4958494537637805, 'feature_fraction_bynode': 0.29734401540052524, 'lambda_l1': 0.12439256238842593, 'lambda_l2': 8.961294442396171e-07, 'learning_rate': 0.07573409480534823, 'num_leaves': 171}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 03:22:34,754]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 03:22:59,812]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 513.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 03:23:31,166]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 241.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:12:49,834]\u001b[0m Trial 16 finished with value: 0.44655801306878173 and parameters: {'drop_rate': 0.12056000882100298, 'max_drop': 28, 'skip_drop': 0.6748802446025792, 'max_depth': 4, 'min_child_samples': 61, 'bagging_fraction': 0.9056795661220721, 'bagging_freq': 14, 'feature_fraction': 0.40489075874355906, 'feature_fraction_bynode': 0.8899985511135988, 'lambda_l1': 4.0713536995880145e-07, 'lambda_l2': 1.1325668984828679e-05, 'learning_rate': 0.09788017432108412, 'num_leaves': 94}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:13:03,580]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:13:16,971]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:13:31,280]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:13:44,606]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:13:58,415]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:27:57,927]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 8435.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:28:11,860]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:28:25,357]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 04:28:39,277]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 05:36:43,940]\u001b[0m Trial 26 finished with value: 0.4436027542646258 and parameters: {'drop_rate': 0.1867815005984771, 'max_drop': 62, 'skip_drop': 0.6016760121131917, 'max_depth': 4, 'min_child_samples': 17, 'bagging_fraction': 0.9392976499117283, 'bagging_freq': 1, 'feature_fraction': 0.735992330539355, 'feature_fraction_bynode': 0.2801653273965533, 'lambda_l1': 0.002382617168324107, 'lambda_l2': 0.0007884347714710849, 'learning_rate': 0.09059352781815762, 'num_leaves': 128}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 05:43:12,216]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 4925.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 05:43:26,577]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 29.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 05:43:40,274]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 05:43:53,624]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 07:01:36,920]\u001b[0m Trial 31 finished with value: 0.4436717772267535 and parameters: {'drop_rate': 0.18885435378860205, 'max_drop': 76, 'skip_drop': 0.5941813161935943, 'max_depth': 4, 'min_child_samples': 9, 'bagging_fraction': 0.9383484959512245, 'bagging_freq': 0, 'feature_fraction': 0.7832247119260194, 'feature_fraction_bynode': 0.2899935047605414, 'lambda_l1': 0.1483345256644823, 'lambda_l2': 8.065283187489985e-06, 'learning_rate': 0.0795057534638061, 'num_leaves': 171}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-20 08:04:22,692]\u001b[0m Trial 32 finished with value: 0.44389251237472305 and parameters: {'drop_rate': 0.1440673701858478, 'max_drop': 66, 'skip_drop': 0.6622209230987822, 'max_depth': 4, 'min_child_samples': 16, 'bagging_fraction': 0.86590874727972, 'bagging_freq': 0, 'feature_fraction': 0.7805127795308324, 'feature_fraction_bynode': 0.48220313238835577, 'lambda_l1': 3.5791642225370223e-06, 'lambda_l2': 1.2247924731252938e-05, 'learning_rate': 0.08173341850235179, 'num_leaves': 114}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 09:00:27,198]\u001b[0m Trial 33 finished with value: 0.4447545855311624 and parameters: {'drop_rate': 0.14174915926352907, 'max_drop': 64, 'skip_drop': 0.724769585742489, 'max_depth': 4, 'min_child_samples': 17, 'bagging_fraction': 0.8604899215822286, 'bagging_freq': 0, 'feature_fraction': 0.7841155843619241, 'feature_fraction_bynode': 0.5856414683402598, 'lambda_l1': 3.937461138712841e-06, 'lambda_l2': 6.551701775406122e-05, 'learning_rate': 0.08302987588701322, 'num_leaves': 505}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:32:21,295]\u001b[0m Trial 34 finished with value: 0.4469201854832165 and parameters: {'drop_rate': 0.19201392100068004, 'max_drop': 73, 'skip_drop': 0.652770213642052, 'max_depth': 5, 'min_child_samples': 8, 'bagging_fraction': 0.8754789586444441, 'bagging_freq': 0, 'feature_fraction': 0.8441626222322395, 'feature_fraction_bynode': 0.5047511091354573, 'lambda_l1': 0.0077385796259246825, 'lambda_l2': 0.00023285071449530818, 'learning_rate': 0.0681476914536799, 'num_leaves': 443}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:32:34,424]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:32:47,005]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:45:06,667]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 8955.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:45:20,176]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:45:33,490]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:45:46,750]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:46:00,803]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:46:16,840]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 64.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:46:30,609]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:46:44,133]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:46:57,597]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:47:11,023]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:50:24,643]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 2625.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 10:50:40,458]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 70.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 11:50:38,886]\u001b[0m Trial 49 finished with value: 0.4442598463463671 and parameters: {'drop_rate': 0.2400275643010724, 'max_drop': 55, 'skip_drop': 0.6515828451580035, 'max_depth': 4, 'min_child_samples': 10, 'bagging_fraction': 0.9970446089014376, 'bagging_freq': 6, 'feature_fraction': 0.73081701995968, 'feature_fraction_bynode': 0.510730041722706, 'lambda_l1': 3.463724900026903e-06, 'lambda_l2': 4.1317549056608043e-07, 'learning_rate': 0.0865349397679474, 'num_leaves': 255}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 11:50:53,482]\u001b[0m Trial 50 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 11:51:06,530]\u001b[0m Trial 51 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 11:51:19,815]\u001b[0m Trial 52 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 11:51:33,666]\u001b[0m Trial 53 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 11:51:51,343]\u001b[0m Trial 54 pruned. Trial was pruned at iteration 82.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:01:14,503]\u001b[0m Trial 55 pruned. Trial was pruned at iteration 6799.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:01:29,378]\u001b[0m Trial 56 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:01:42,537]\u001b[0m Trial 57 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:01:55,908]\u001b[0m Trial 58 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:02:08,776]\u001b[0m Trial 59 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:02:22,136]\u001b[0m Trial 60 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:02:41,775]\u001b[0m Trial 61 pruned. Trial was pruned at iteration 116.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 12:59:11,765]\u001b[0m Trial 62 finished with value: 0.4434788574360626 and parameters: {'drop_rate': 0.24043668353465833, 'max_drop': 66, 'skip_drop': 0.6514692613953877, 'max_depth': 4, 'min_child_samples': 6, 'bagging_fraction': 0.9411132009080923, 'bagging_freq': 6, 'feature_fraction': 0.7203651272934557, 'feature_fraction_bynode': 0.48676799782012975, 'lambda_l1': 0.20502022705245526, 'lambda_l2': 7.822292280418188e-07, 'learning_rate': 0.08738392568470704, 'num_leaves': 247}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 13:09:33,867]\u001b[0m Trial 63 pruned. Trial was pruned at iteration 9428.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 13:09:46,616]\u001b[0m Trial 64 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 14:06:39,457]\u001b[0m Trial 65 finished with value: 0.4439235196010717 and parameters: {'drop_rate': 0.26235669081312085, 'max_drop': 60, 'skip_drop': 0.5900920959286166, 'max_depth': 4, 'min_child_samples': 4, 'bagging_fraction': 0.8921439777129103, 'bagging_freq': 10, 'feature_fraction': 0.7014282684557096, 'feature_fraction_bynode': 0.3632090778794489, 'lambda_l1': 0.34094355021067285, 'lambda_l2': 0.0003538607013744811, 'learning_rate': 0.0895251391309995, 'num_leaves': 486}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 14:06:52,400]\u001b[0m Trial 66 pruned. Trial was pruned at iteration 5.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:03:25,318]\u001b[0m Trial 67 finished with value: 0.4451757200310274 and parameters: {'drop_rate': 0.2828245425182156, 'max_drop': 58, 'skip_drop': 0.5937576288171267, 'max_depth': 4, 'min_child_samples': 12, 'bagging_fraction': 0.8917360852083684, 'bagging_freq': 13, 'feature_fraction': 0.8222481973897681, 'feature_fraction_bynode': 0.548465699444036, 'lambda_l1': 0.2742391980070333, 'lambda_l2': 0.007519572730719265, 'learning_rate': 0.09973032390949386, 'num_leaves': 488}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:03:38,149]\u001b[0m Trial 68 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:07:41,057]\u001b[0m Trial 69 pruned. Trial was pruned at iteration 5098.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:07:57,799]\u001b[0m Trial 70 pruned. Trial was pruned at iteration 70.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:08:10,012]\u001b[0m Trial 71 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:08:22,473]\u001b[0m Trial 72 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:08:34,688]\u001b[0m Trial 73 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:08:47,103]\u001b[0m Trial 74 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:08:59,099]\u001b[0m Trial 75 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:09:11,390]\u001b[0m Trial 76 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:09:25,973]\u001b[0m Trial 77 pruned. Trial was pruned at iteration 66.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 15:09:38,309]\u001b[0m Trial 78 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 16:13:24,512]\u001b[0m Trial 79 finished with value: 0.44347972831220595 and parameters: {'drop_rate': 0.09796037688072537, 'max_drop': 91, 'skip_drop': 0.6800244447622579, 'max_depth': 4, 'min_child_samples': 10, 'bagging_fraction': 0.8826396624812333, 'bagging_freq': 11, 'feature_fraction': 0.595113279961342, 'feature_fraction_bynode': 0.44193777202607404, 'lambda_l1': 0.014792041559879588, 'lambda_l2': 7.629847140653839e-05, 'learning_rate': 0.08314247609432765, 'num_leaves': 270}. Best is trial 8 with value: 0.4434747376662068.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 16:21:35,217]\u001b[0m Trial 80 pruned. Trial was pruned at iteration 7225.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 17:18:48,184]\u001b[0m Trial 81 finished with value: 0.4432953622815359 and parameters: {'drop_rate': 0.04181725177738285, 'max_drop': 90, 'skip_drop': 0.681433217444748, 'max_depth': 4, 'min_child_samples': 10, 'bagging_fraction': 0.879149476519461, 'bagging_freq': 10, 'feature_fraction': 0.6338351071617777, 'feature_fraction_bynode': 0.4578052622858927, 'lambda_l1': 0.09329820302670715, 'lambda_l2': 0.00038402177706728873, 'learning_rate': 0.08343032559963369, 'num_leaves': 264}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-20 17:23:36,748]\u001b[0m Trial 82 pruned. Trial was pruned at iteration 4250.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 17:23:49,410]\u001b[0m Trial 83 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 18:20:50,323]\u001b[0m Trial 84 finished with value: 0.4436394078041911 and parameters: {'drop_rate': 0.048255846591648714, 'max_drop': 95, 'skip_drop': 0.6702823806497216, 'max_depth': 4, 'min_child_samples': 66, 'bagging_fraction': 0.9080695222862849, 'bagging_freq': 11, 'feature_fraction': 0.6689573953951344, 'feature_fraction_bynode': 0.4929108700285945, 'lambda_l1': 0.1098053462108551, 'lambda_l2': 0.0004503009357987117, 'learning_rate': 0.08377740123144017, 'num_leaves': 214}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 19:24:13,602]\u001b[0m Trial 85 finished with value: 0.44390589444963025 and parameters: {'drop_rate': 0.08010107182284039, 'max_drop': 90, 'skip_drop': 0.6776322579650669, 'max_depth': 4, 'min_child_samples': 69, 'bagging_fraction': 0.9143687704123559, 'bagging_freq': 12, 'feature_fraction': 0.6706714021640159, 'feature_fraction_bynode': 0.491767062921478, 'lambda_l1': 0.0945981245790216, 'lambda_l2': 0.0004235555757348431, 'learning_rate': 0.08416815717661287, 'num_leaves': 223}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 19:24:27,066]\u001b[0m Trial 86 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:26:54,517]\u001b[0m Trial 87 finished with value: 0.44475492437049347 and parameters: {'drop_rate': 0.0397180705605055, 'max_drop': 95, 'skip_drop': 0.6888330639368265, 'max_depth': 4, 'min_child_samples': 59, 'bagging_fraction': 0.8131407129582795, 'bagging_freq': 13, 'feature_fraction': 0.7414489536041745, 'feature_fraction_bynode': 0.5780703594471377, 'lambda_l1': 0.09815144421705464, 'lambda_l2': 0.004788877443004738, 'learning_rate': 0.08459648049344427, 'num_leaves': 273}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:27:07,842]\u001b[0m Trial 88 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:27:20,553]\u001b[0m Trial 89 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:27:34,007]\u001b[0m Trial 90 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:29:25,908]\u001b[0m Trial 91 pruned. Trial was pruned at iteration 1892.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:36:21,811]\u001b[0m Trial 92 pruned. Trial was pruned at iteration 6115.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:36:34,843]\u001b[0m Trial 93 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:43:24,429]\u001b[0m Trial 94 pruned. Trial was pruned at iteration 5558.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:43:38,033]\u001b[0m Trial 95 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 20:43:54,781]\u001b[0m Trial 96 pruned. Trial was pruned at iteration 75.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:27:01,582]\u001b[0m Trial 97 finished with value: 0.4442039349044059 and parameters: {'drop_rate': 0.03482674792026908, 'max_drop': 57, 'skip_drop': 0.7501193888891656, 'max_depth': 4, 'min_child_samples': 54, 'bagging_fraction': 0.9586711269866999, 'bagging_freq': 9, 'feature_fraction': 0.6685560893132136, 'feature_fraction_bynode': 0.6081639246254537, 'lambda_l1': 0.07602940568183904, 'lambda_l2': 0.0039378067345240434, 'learning_rate': 0.08696403494505116, 'num_leaves': 222}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:27:15,636]\u001b[0m Trial 98 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:27:28,765]\u001b[0m Trial 99 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:27:42,058]\u001b[0m Trial 100 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:27:57,965]\u001b[0m Trial 101 pruned. Trial was pruned at iteration 62.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:28:13,673]\u001b[0m Trial 102 pruned. Trial was pruned at iteration 60.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:28:26,863]\u001b[0m Trial 103 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:28:40,216]\u001b[0m Trial 104 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:28:53,502]\u001b[0m Trial 105 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:29:06,647]\u001b[0m Trial 106 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:29:19,702]\u001b[0m Trial 107 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:29:33,266]\u001b[0m Trial 108 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:29:46,657]\u001b[0m Trial 109 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:30:00,167]\u001b[0m Trial 110 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:30:13,317]\u001b[0m Trial 111 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:30:26,808]\u001b[0m Trial 112 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:30:39,924]\u001b[0m Trial 113 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:30:53,213]\u001b[0m Trial 114 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:31:06,608]\u001b[0m Trial 115 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:31:19,909]\u001b[0m Trial 116 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:31:34,311]\u001b[0m Trial 117 pruned. Trial was pruned at iteration 29.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:31:49,785]\u001b[0m Trial 118 pruned. Trial was pruned at iteration 51.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:32:03,070]\u001b[0m Trial 119 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 21:32:16,410]\u001b[0m Trial 120 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 22:16:27,257]\u001b[0m Trial 121 finished with value: 0.44492457510623395 and parameters: {'drop_rate': 0.036716298134788, 'max_drop': 57, 'skip_drop': 0.7537761164586816, 'max_depth': 4, 'min_child_samples': 68, 'bagging_fraction': 0.9592587845487024, 'bagging_freq': 9, 'feature_fraction': 0.668408250073088, 'feature_fraction_bynode': 0.7308262760541167, 'lambda_l1': 0.10021461604856524, 'lambda_l2': 0.0061240144185527685, 'learning_rate': 0.0857787754723713, 'num_leaves': 221}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 22:16:42,004]\u001b[0m Trial 122 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 23:08:48,216]\u001b[0m Trial 123 finished with value: 0.44448468675207825 and parameters: {'drop_rate': 0.04623193204126415, 'max_drop': 49, 'skip_drop': 0.6949188837782336, 'max_depth': 4, 'min_child_samples': 52, 'bagging_fraction': 0.9023457114190009, 'bagging_freq': 8, 'feature_fraction': 0.6991230667943703, 'feature_fraction_bynode': 0.5157187707256857, 'lambda_l1': 0.06131553676198525, 'lambda_l2': 0.003576126056247503, 'learning_rate': 0.0877231009050936, 'num_leaves': 189}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-20 23:09:01,943]\u001b[0m Trial 124 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 00:00:56,526]\u001b[0m Trial 125 finished with value: 0.4459678121888245 and parameters: {'drop_rate': 0.15507255155375443, 'max_drop': 56, 'skip_drop': 0.7953734882085466, 'max_depth': 4, 'min_child_samples': 62, 'bagging_fraction': 0.9689746896140827, 'bagging_freq': 8, 'feature_fraction': 0.6075347682556937, 'feature_fraction_bynode': 0.6869246004222074, 'lambda_l1': 1.3208420716384042, 'lambda_l2': 0.0006410516910844279, 'learning_rate': 0.0843717962179923, 'num_leaves': 144}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 00:59:00,565]\u001b[0m Trial 126 finished with value: 0.44552800181530045 and parameters: {'drop_rate': 0.0594142305625969, 'max_drop': 58, 'skip_drop': 0.7372020177133977, 'max_depth': 4, 'min_child_samples': 41, 'bagging_fraction': 0.8752225417849822, 'bagging_freq': 10, 'feature_fraction': 0.5884410106015523, 'feature_fraction_bynode': 0.45148594354389127, 'lambda_l1': 0.17500053933834536, 'lambda_l2': 9.132996894395733e-05, 'learning_rate': 0.09037800180460907, 'num_leaves': 474}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 00:59:15,022]\u001b[0m Trial 127 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 00:59:47,901]\u001b[0m Trial 128 pruned. Trial was pruned at iteration 299.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:00:05,377]\u001b[0m Trial 129 pruned. Trial was pruned at iteration 73.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:00:20,197]\u001b[0m Trial 130 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 01:00:34,345]\u001b[0m Trial 131 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:00:48,822]\u001b[0m Trial 132 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:01:02,789]\u001b[0m Trial 133 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:07:31,750]\u001b[0m Trial 134 pruned. Trial was pruned at iteration 7941.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:07:46,557]\u001b[0m Trial 135 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:11:18,844]\u001b[0m Trial 136 pruned. Trial was pruned at iteration 4518.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:11:39,732]\u001b[0m Trial 137 pruned. Trial was pruned at iteration 197.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:11:53,167]\u001b[0m Trial 138 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:12:06,811]\u001b[0m Trial 139 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:12:19,939]\u001b[0m Trial 140 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 01:18:00,277]\u001b[0m Trial 141 pruned. Trial was pruned at iteration 6811.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 02:05:47,905]\u001b[0m Trial 142 finished with value: 0.4447085005647858 and parameters: {'drop_rate': 0.03785626219098802, 'max_drop': 50, 'skip_drop': 0.7063853267858171, 'max_depth': 4, 'min_child_samples': 49, 'bagging_fraction': 0.8808782688271249, 'bagging_freq': 8, 'feature_fraction': 0.6729106693260027, 'feature_fraction_bynode': 0.5262100693580767, 'lambda_l1': 0.028081801086467576, 'lambda_l2': 0.008070482141043988, 'learning_rate': 0.08731662890263749, 'num_leaves': 194}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 02:06:02,394]\u001b[0m Trial 143 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 02:55:28,023]\u001b[0m Trial 144 finished with value: 0.4436877426385005 and parameters: {'drop_rate': 0.06259124632827912, 'max_drop': 59, 'skip_drop': 0.7204993794909532, 'max_depth': 4, 'min_child_samples': 3, 'bagging_fraction': 0.9465767395414899, 'bagging_freq': 8, 'feature_fraction': 0.7289222737305313, 'feature_fraction_bynode': 0.4586688738271315, 'lambda_l1': 0.05908979728275047, 'lambda_l2': 0.001779757428121802, 'learning_rate': 0.08600900952560941, 'num_leaves': 156}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 03:41:35,786]\u001b[0m Trial 145 finished with value: 0.44494644899225244 and parameters: {'drop_rate': 0.06153416542117331, 'max_drop': 62, 'skip_drop': 0.7261722829647291, 'max_depth': 4, 'min_child_samples': 2, 'bagging_fraction': 0.9513211383535491, 'bagging_freq': 9, 'feature_fraction': 0.7310944141584693, 'feature_fraction_bynode': 0.4442911306439674, 'lambda_l1': 2.1761860685032963e-06, 'lambda_l2': 0.0017501965578724345, 'learning_rate': 0.09334381664983336, 'num_leaves': 139}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:33:07,601]\u001b[0m Trial 146 finished with value: 0.4435953059833853 and parameters: {'drop_rate': 0.030210290441430174, 'max_drop': 59, 'skip_drop': 0.7399546617269953, 'max_depth': 4, 'min_child_samples': 3, 'bagging_fraction': 0.9834544940993046, 'bagging_freq': 7, 'feature_fraction': 0.5331103056427205, 'feature_fraction_bynode': 0.4306131794189924, 'lambda_l1': 0.35252475285821283, 'lambda_l2': 0.0010688894473526388, 'learning_rate': 0.08431881999903693, 'num_leaves': 114}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:33:20,678]\u001b[0m Trial 147 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:33:33,014]\u001b[0m Trial 148 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:33:45,612]\u001b[0m Trial 149 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:33:57,998]\u001b[0m Trial 150 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:35:49,812]\u001b[0m Trial 151 pruned. Trial was pruned at iteration 2121.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:36:02,307]\u001b[0m Trial 152 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:36:14,981]\u001b[0m Trial 153 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:36:27,569]\u001b[0m Trial 154 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:36:43,629]\u001b[0m Trial 155 pruned. Trial was pruned at iteration 93.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:36:56,197]\u001b[0m Trial 156 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:37:19,831]\u001b[0m Trial 157 pruned. Trial was pruned at iteration 284.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:37:32,938]\u001b[0m Trial 158 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:37:45,254]\u001b[0m Trial 159 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:37:57,863]\u001b[0m Trial 160 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:42:55,306]\u001b[0m Trial 161 pruned. Trial was pruned at iteration 5612.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:43:08,176]\u001b[0m Trial 162 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:44:45,316]\u001b[0m Trial 163 pruned. Trial was pruned at iteration 1786.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:44:58,005]\u001b[0m Trial 164 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:45:10,464]\u001b[0m Trial 165 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:49:35,848]\u001b[0m Trial 166 pruned. Trial was pruned at iteration 6057.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:49:48,523]\u001b[0m Trial 167 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:50:01,820]\u001b[0m Trial 168 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:50:14,387]\u001b[0m Trial 169 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:50:27,168]\u001b[0m Trial 170 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:50:39,736]\u001b[0m Trial 171 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:50:52,350]\u001b[0m Trial 172 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:51:05,030]\u001b[0m Trial 173 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:51:18,292]\u001b[0m Trial 174 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:51:30,974]\u001b[0m Trial 175 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:51:43,508]\u001b[0m Trial 176 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:51:56,182]\u001b[0m Trial 177 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:52:08,881]\u001b[0m Trial 178 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:52:22,021]\u001b[0m Trial 179 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:52:34,601]\u001b[0m Trial 180 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:52:47,170]\u001b[0m Trial 181 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:52:59,964]\u001b[0m Trial 182 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:53:12,744]\u001b[0m Trial 183 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:53:25,329]\u001b[0m Trial 184 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:53:37,949]\u001b[0m Trial 185 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:53:50,599]\u001b[0m Trial 186 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:54:03,205]\u001b[0m Trial 187 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:54:54,037]\u001b[0m Trial 188 pruned. Trial was pruned at iteration 913.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:59:18,441]\u001b[0m Trial 189 pruned. Trial was pruned at iteration 4228.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:59:31,224]\u001b[0m Trial 190 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:59:43,717]\u001b[0m Trial 191 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 04:59:56,343]\u001b[0m Trial 192 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:00:09,031]\u001b[0m Trial 193 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:03:16,694]\u001b[0m Trial 194 pruned. Trial was pruned at iteration 3884.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:03:29,171]\u001b[0m Trial 195 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:03:41,836]\u001b[0m Trial 196 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:31:23,856]\u001b[0m Trial 197 finished with value: 0.44833064583628524 and parameters: {'drop_rate': 0.056272900730562084, 'max_drop': 7, 'skip_drop': 0.724890877817938, 'max_depth': 4, 'min_child_samples': 26, 'bagging_fraction': 0.8660986064421271, 'bagging_freq': 9, 'feature_fraction': 0.7388947111954853, 'feature_fraction_bynode': 0.48133324529763855, 'lambda_l1': 0.0008623760109972541, 'lambda_l2': 0.0004336999201866366, 'learning_rate': 0.09540425074073029, 'num_leaves': 408}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 05:31:37,328]\u001b[0m Trial 198 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:31:49,991]\u001b[0m Trial 199 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:32:03,092]\u001b[0m Trial 200 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:32:15,999]\u001b[0m Trial 201 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:32:29,205]\u001b[0m Trial 202 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 05:32:42,232]\u001b[0m Trial 203 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:17:52,759]\u001b[0m Trial 204 finished with value: 0.4449208373486937 and parameters: {'drop_rate': 0.044255776208326485, 'max_drop': 64, 'skip_drop': 0.7365418515953888, 'max_depth': 4, 'min_child_samples': 10, 'bagging_fraction': 0.856502417367178, 'bagging_freq': 0, 'feature_fraction': 0.6967291566240559, 'feature_fraction_bynode': 0.6275613411853981, 'lambda_l1': 1.182994778589365e-06, 'lambda_l2': 5.484023928291786e-05, 'learning_rate': 0.09065266400850597, 'num_leaves': 492}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:18:06,065]\u001b[0m Trial 205 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:18:18,640]\u001b[0m Trial 206 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:18:31,656]\u001b[0m Trial 207 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:18:44,405]\u001b[0m Trial 208 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:18:57,454]\u001b[0m Trial 209 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:19:10,320]\u001b[0m Trial 210 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:19:23,346]\u001b[0m Trial 211 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:19:45,875]\u001b[0m Trial 212 pruned. Trial was pruned at iteration 261.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:19:59,183]\u001b[0m Trial 213 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:20:11,709]\u001b[0m Trial 214 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:20:24,739]\u001b[0m Trial 215 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:24:46,168]\u001b[0m Trial 216 pruned. Trial was pruned at iteration 4734.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:24:59,094]\u001b[0m Trial 217 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:25:11,764]\u001b[0m Trial 218 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:25:24,816]\u001b[0m Trial 219 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:25:37,651]\u001b[0m Trial 220 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 06:31:01,250]\u001b[0m Trial 221 pruned. Trial was pruned at iteration 6355.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 07:17:12,895]\u001b[0m Trial 222 finished with value: 0.44450802529578654 and parameters: {'drop_rate': 0.04340979810259107, 'max_drop': 64, 'skip_drop': 0.7195956166089454, 'max_depth': 4, 'min_child_samples': 14, 'bagging_fraction': 0.8566773365865761, 'bagging_freq': 0, 'feature_fraction': 0.7095253494875298, 'feature_fraction_bynode': 0.6038885324787997, 'lambda_l1': 0.03351040606799648, 'lambda_l2': 0.00014069210164924635, 'learning_rate': 0.08984465294993993, 'num_leaves': 493}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 07:19:55,925]\u001b[0m Trial 223 pruned. Trial was pruned at iteration 3006.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 08:07:19,737]\u001b[0m Trial 224 finished with value: 0.44402129005508134 and parameters: {'drop_rate': 0.06093926015737309, 'max_drop': 67, 'skip_drop': 0.7249825648516666, 'max_depth': 4, 'min_child_samples': 15, 'bagging_fraction': 0.9071827199856676, 'bagging_freq': 0, 'feature_fraction': 0.7116362528300609, 'feature_fraction_bynode': 0.6057079069128898, 'lambda_l1': 0.03903578441139917, 'lambda_l2': 0.0049447260523317205, 'learning_rate': 0.08402601464497977, 'num_leaves': 318}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 08:54:16,743]\u001b[0m Trial 225 finished with value: 0.4446681625536485 and parameters: {'drop_rate': 0.062064851732397966, 'max_drop': 66, 'skip_drop': 0.7302188377582532, 'max_depth': 4, 'min_child_samples': 17, 'bagging_fraction': 0.908003118882501, 'bagging_freq': 0, 'feature_fraction': 0.7124546506801527, 'feature_fraction_bynode': 0.6463604720680214, 'lambda_l1': 0.029161040515027055, 'lambda_l2': 0.00021345687975747493, 'learning_rate': 0.08787517791964772, 'num_leaves': 315}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 08:54:29,696]\u001b[0m Trial 226 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 09:42:04,690]\u001b[0m Trial 227 finished with value: 0.4452809671045369 and parameters: {'drop_rate': 0.054610106984407124, 'max_drop': 66, 'skip_drop': 0.7664162729056015, 'max_depth': 4, 'min_child_samples': 13, 'bagging_fraction': 0.9450851306883722, 'bagging_freq': 0, 'feature_fraction': 0.6771544288655893, 'feature_fraction_bynode': 0.4873807655014285, 'lambda_l1': 0.03038469783491349, 'lambda_l2': 0.00013002652256786324, 'learning_rate': 0.09295661905283638, 'num_leaves': 310}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 09:42:18,781]\u001b[0m Trial 228 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 10:22:26,071]\u001b[0m Trial 229 finished with value: 0.4446382377670581 and parameters: {'drop_rate': 0.05712647647344503, 'max_drop': 25, 'skip_drop': 0.7286609503192114, 'max_depth': 4, 'min_child_samples': 11, 'bagging_fraction': 0.8982526009809958, 'bagging_freq': 1, 'feature_fraction': 0.6594547967519814, 'feature_fraction_bynode': 0.6079058639400559, 'lambda_l1': 0.03807840436577431, 'lambda_l2': 0.0006271966560337114, 'learning_rate': 0.08709136922969496, 'num_leaves': 354}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 11:20:38,471]\u001b[0m Trial 230 finished with value: 0.4452630782105211 and parameters: {'drop_rate': 0.05931345758276297, 'max_drop': 71, 'skip_drop': 0.758451150751212, 'max_depth': 4, 'min_child_samples': 11, 'bagging_fraction': 0.9025460921074542, 'bagging_freq': 1, 'feature_fraction': 0.6238087690164384, 'feature_fraction_bynode': 0.6006951433712171, 'lambda_l1': 0.04061218149480295, 'lambda_l2': 0.00039946427926605736, 'learning_rate': 0.0889538008040025, 'num_leaves': 385}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:18:38,726]\u001b[0m Trial 231 finished with value: 0.44442218100471703 and parameters: {'drop_rate': 0.06793551637560984, 'max_drop': 65, 'skip_drop': 0.7235965331450546, 'max_depth': 4, 'min_child_samples': 12, 'bagging_fraction': 0.8931555188702412, 'bagging_freq': 0, 'feature_fraction': 0.6577463982408809, 'feature_fraction_bynode': 0.6446628901854483, 'lambda_l1': 0.022524949110287917, 'lambda_l2': 0.0015391893273872682, 'learning_rate': 0.08732021576366807, 'num_leaves': 338}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:18:53,051]\u001b[0m Trial 232 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:55:52,759]\u001b[0m Trial 233 finished with value: 0.44507726183108004 and parameters: {'drop_rate': 0.05762145424224415, 'max_drop': 12, 'skip_drop': 0.7311009608093014, 'max_depth': 4, 'min_child_samples': 16, 'bagging_fraction': 0.9160530221223949, 'bagging_freq': 0, 'feature_fraction': 0.6384141717151364, 'feature_fraction_bynode': 0.6115082149850776, 'lambda_l1': 0.009051870287522277, 'lambda_l2': 0.00185960054496717, 'learning_rate': 0.08581316411269008, 'num_leaves': 344}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:56:09,941]\u001b[0m Trial 234 pruned. Trial was pruned at iteration 86.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:56:24,271]\u001b[0m Trial 235 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:56:38,289]\u001b[0m Trial 236 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:56:52,234]\u001b[0m Trial 237 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:57:08,948]\u001b[0m Trial 238 pruned. Trial was pruned at iteration 68.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:57:22,857]\u001b[0m Trial 239 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:57:38,521]\u001b[0m Trial 240 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 12:57:51,903]\u001b[0m Trial 241 pruned. Trial was pruned at iteration 1.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 13:02:18,161]\u001b[0m Trial 242 pruned. Trial was pruned at iteration 4033.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:02:32,370]\u001b[0m Trial 243 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:02:45,886]\u001b[0m Trial 244 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:02:59,539]\u001b[0m Trial 245 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:03:13,061]\u001b[0m Trial 246 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:03:26,590]\u001b[0m Trial 247 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:03:40,109]\u001b[0m Trial 248 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:03:53,877]\u001b[0m Trial 249 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:04:07,747]\u001b[0m Trial 250 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:04:22,283]\u001b[0m Trial 251 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:04:37,878]\u001b[0m Trial 252 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:04:51,925]\u001b[0m Trial 253 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:05:06,301]\u001b[0m Trial 254 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:09:28,758]\u001b[0m Trial 255 pruned. Trial was pruned at iteration 3955.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:09:41,810]\u001b[0m Trial 256 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:09:54,532]\u001b[0m Trial 257 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:10:07,622]\u001b[0m Trial 258 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:10:20,343]\u001b[0m Trial 259 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:10:33,344]\u001b[0m Trial 260 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:10:46,238]\u001b[0m Trial 261 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:10:59,808]\u001b[0m Trial 262 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:11:12,671]\u001b[0m Trial 263 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:11:25,549]\u001b[0m Trial 264 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:11:38,642]\u001b[0m Trial 265 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:11:51,649]\u001b[0m Trial 266 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:12:04,292]\u001b[0m Trial 267 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:12:17,377]\u001b[0m Trial 268 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:12:30,196]\u001b[0m Trial 269 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:53:22,229]\u001b[0m Trial 270 finished with value: 0.4454612926132622 and parameters: {'drop_rate': 0.03989212846336619, 'max_drop': 20, 'skip_drop': 0.7223894798985985, 'max_depth': 4, 'min_child_samples': 18, 'bagging_fraction': 0.5838573315645417, 'bagging_freq': 0, 'feature_fraction': 0.9594232403846685, 'feature_fraction_bynode': 0.6319809060354692, 'lambda_l1': 5.5253060260177945e-05, 'lambda_l2': 0.0006056470532323098, 'learning_rate': 0.08628409105902471, 'num_leaves': 163}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:53:36,120]\u001b[0m Trial 271 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:53:51,972]\u001b[0m Trial 272 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:54:05,848]\u001b[0m Trial 273 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:54:19,412]\u001b[0m Trial 274 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:54:33,145]\u001b[0m Trial 275 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:54:47,657]\u001b[0m Trial 276 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:55:01,243]\u001b[0m Trial 277 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:55:14,738]\u001b[0m Trial 278 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:55:28,608]\u001b[0m Trial 279 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:55:42,265]\u001b[0m Trial 280 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:55:55,790]\u001b[0m Trial 281 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:56:09,373]\u001b[0m Trial 282 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:56:23,091]\u001b[0m Trial 283 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 13:56:36,639]\u001b[0m Trial 284 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:36:29,360]\u001b[0m Trial 285 finished with value: 0.44610827323914065 and parameters: {'drop_rate': 0.034882390962405654, 'max_drop': 30, 'skip_drop': 0.7519986612095084, 'max_depth': 4, 'min_child_samples': 16, 'bagging_fraction': 0.8455311020133549, 'bagging_freq': 11, 'feature_fraction': 0.6955444272142703, 'feature_fraction_bynode': 0.6431596816795095, 'lambda_l1': 0.25867315695785964, 'lambda_l2': 1.811593845072408e-05, 'learning_rate': 0.09005278310361389, 'num_leaves': 235}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:36:43,326]\u001b[0m Trial 286 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:36:57,100]\u001b[0m Trial 287 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:37:10,946]\u001b[0m Trial 288 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:37:24,261]\u001b[0m Trial 289 pruned. Trial was pruned at iteration 2.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:37:39,213]\u001b[0m Trial 290 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:37:52,730]\u001b[0m Trial 291 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:38:06,348]\u001b[0m Trial 292 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:38:20,375]\u001b[0m Trial 293 pruned. Trial was pruned at iteration 14.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:38:34,304]\u001b[0m Trial 294 pruned. Trial was pruned at iteration 7.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:38:47,545]\u001b[0m Trial 295 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:39:02,855]\u001b[0m Trial 296 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:39:16,218]\u001b[0m Trial 297 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:39:30,072]\u001b[0m Trial 298 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:39:44,151]\u001b[0m Trial 299 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:39:59,731]\u001b[0m Trial 300 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:40:13,437]\u001b[0m Trial 301 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:40:27,310]\u001b[0m Trial 302 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:40:40,989]\u001b[0m Trial 303 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:40:56,341]\u001b[0m Trial 304 pruned. Trial was pruned at iteration 17.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:41:09,811]\u001b[0m Trial 305 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:41:23,572]\u001b[0m Trial 306 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:41:37,149]\u001b[0m Trial 307 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:41:52,516]\u001b[0m Trial 308 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:42:07,026]\u001b[0m Trial 309 pruned. Trial was pruned at iteration 7.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:42:20,650]\u001b[0m Trial 310 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:42:34,915]\u001b[0m Trial 311 pruned. Trial was pruned at iteration 25.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:42:48,690]\u001b[0m Trial 312 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:43:02,024]\u001b[0m Trial 313 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:43:16,238]\u001b[0m Trial 314 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:43:30,003]\u001b[0m Trial 315 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:43:44,710]\u001b[0m Trial 316 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:43:58,095]\u001b[0m Trial 317 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:44:11,527]\u001b[0m Trial 318 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:44:25,197]\u001b[0m Trial 319 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:44:38,769]\u001b[0m Trial 320 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 14:44:52,014]\u001b[0m Trial 321 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:45:05,821]\u001b[0m Trial 322 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:45:19,474]\u001b[0m Trial 323 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:45:33,129]\u001b[0m Trial 324 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:45:46,491]\u001b[0m Trial 325 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:45:59,966]\u001b[0m Trial 326 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:46:13,823]\u001b[0m Trial 327 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:51:58,097]\u001b[0m Trial 328 pruned. Trial was pruned at iteration 6183.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:52:11,710]\u001b[0m Trial 329 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:52:25,912]\u001b[0m Trial 330 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:52:39,665]\u001b[0m Trial 331 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:52:55,077]\u001b[0m Trial 332 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:53:08,943]\u001b[0m Trial 333 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:53:22,483]\u001b[0m Trial 334 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 14:53:36,286]\u001b[0m Trial 335 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:26:35,270]\u001b[0m Trial 336 finished with value: 0.4465858195863529 and parameters: {'drop_rate': 0.040209393280605285, 'max_drop': 11, 'skip_drop': 0.7443705845117017, 'max_depth': 4, 'min_child_samples': 6, 'bagging_fraction': 0.9284650163039478, 'bagging_freq': 5, 'feature_fraction': 0.7464638449551709, 'feature_fraction_bynode': 0.6162179023611676, 'lambda_l1': 0.0020528146097015224, 'lambda_l2': 0.008380758092664175, 'learning_rate': 0.09109379654676943, 'num_leaves': 208}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:26:48,854]\u001b[0m Trial 337 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:27:01,762]\u001b[0m Trial 338 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:27:15,907]\u001b[0m Trial 339 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:27:29,608]\u001b[0m Trial 340 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:27:44,282]\u001b[0m Trial 341 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:27:57,881]\u001b[0m Trial 342 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:28:11,959]\u001b[0m Trial 343 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:28:25,847]\u001b[0m Trial 344 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:28:40,933]\u001b[0m Trial 345 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:28:54,702]\u001b[0m Trial 346 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:29:09,381]\u001b[0m Trial 347 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:29:23,265]\u001b[0m Trial 348 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:29:37,140]\u001b[0m Trial 349 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:29:51,371]\u001b[0m Trial 350 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:30:08,426]\u001b[0m Trial 351 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:30:24,837]\u001b[0m Trial 352 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:30:38,953]\u001b[0m Trial 353 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:30:52,462]\u001b[0m Trial 354 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:31:06,327]\u001b[0m Trial 355 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:31:20,774]\u001b[0m Trial 356 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:31:35,488]\u001b[0m Trial 357 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:31:49,608]\u001b[0m Trial 358 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:32:03,552]\u001b[0m Trial 359 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:32:17,951]\u001b[0m Trial 360 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:32:32,517]\u001b[0m Trial 361 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:32:46,494]\u001b[0m Trial 362 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:33:01,788]\u001b[0m Trial 363 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:33:15,429]\u001b[0m Trial 364 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:33:33,031]\u001b[0m Trial 365 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:33:47,172]\u001b[0m Trial 366 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:34:01,476]\u001b[0m Trial 367 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:43:45,890]\u001b[0m Trial 368 pruned. Trial was pruned at iteration 6577.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:43:59,417]\u001b[0m Trial 369 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:44:15,210]\u001b[0m Trial 370 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:44:28,843]\u001b[0m Trial 371 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:44:43,097]\u001b[0m Trial 372 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:44:57,007]\u001b[0m Trial 373 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:45:11,018]\u001b[0m Trial 374 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:45:24,877]\u001b[0m Trial 375 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:45:38,844]\u001b[0m Trial 376 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:45:53,241]\u001b[0m Trial 377 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:46:07,014]\u001b[0m Trial 378 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:46:21,126]\u001b[0m Trial 379 pruned. Trial was pruned at iteration 16.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:46:35,103]\u001b[0m Trial 380 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:46:48,933]\u001b[0m Trial 381 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:47:02,609]\u001b[0m Trial 382 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:47:18,805]\u001b[0m Trial 383 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:47:33,136]\u001b[0m Trial 384 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:47:46,776]\u001b[0m Trial 385 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:48:02,666]\u001b[0m Trial 386 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:48:16,922]\u001b[0m Trial 387 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:48:31,057]\u001b[0m Trial 388 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:48:45,731]\u001b[0m Trial 389 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:48:59,941]\u001b[0m Trial 390 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:49:13,286]\u001b[0m Trial 391 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:49:27,687]\u001b[0m Trial 392 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:49:41,709]\u001b[0m Trial 393 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:49:57,975]\u001b[0m Trial 394 pruned. Trial was pruned at iteration 75.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:50:11,553]\u001b[0m Trial 395 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:50:25,720]\u001b[0m Trial 396 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:50:39,225]\u001b[0m Trial 397 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:50:53,733]\u001b[0m Trial 398 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:51:07,387]\u001b[0m Trial 399 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:51:22,084]\u001b[0m Trial 400 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:51:38,074]\u001b[0m Trial 401 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:51:55,029]\u001b[0m Trial 402 pruned. Trial was pruned at iteration 70.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:52:09,080]\u001b[0m Trial 403 pruned. Trial was pruned at iteration 8.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:53:33,311]\u001b[0m Trial 404 pruned. Trial was pruned at iteration 1387.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 15:53:47,690]\u001b[0m Trial 405 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:54:01,610]\u001b[0m Trial 406 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:54:15,968]\u001b[0m Trial 407 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:54:29,475]\u001b[0m Trial 408 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:54:43,502]\u001b[0m Trial 409 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:54:57,284]\u001b[0m Trial 410 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:55:11,275]\u001b[0m Trial 411 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:55:24,848]\u001b[0m Trial 412 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:55:39,197]\u001b[0m Trial 413 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:55:53,701]\u001b[0m Trial 414 pruned. Trial was pruned at iteration 25.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:56:08,089]\u001b[0m Trial 415 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:56:21,815]\u001b[0m Trial 416 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:56:35,870]\u001b[0m Trial 417 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:56:49,315]\u001b[0m Trial 418 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:57:03,260]\u001b[0m Trial 419 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:57:18,937]\u001b[0m Trial 420 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:57:32,892]\u001b[0m Trial 421 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:57:46,474]\u001b[0m Trial 422 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:58:00,300]\u001b[0m Trial 423 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:58:13,852]\u001b[0m Trial 424 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:58:27,669]\u001b[0m Trial 425 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:58:41,482]\u001b[0m Trial 426 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:58:55,721]\u001b[0m Trial 427 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:59:09,542]\u001b[0m Trial 428 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:59:23,288]\u001b[0m Trial 429 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:59:36,652]\u001b[0m Trial 430 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 15:59:50,390]\u001b[0m Trial 431 pruned. Trial was pruned at iteration 7.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:00:03,435]\u001b[0m Trial 432 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:00:16,851]\u001b[0m Trial 433 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:00:30,158]\u001b[0m Trial 434 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:00:43,968]\u001b[0m Trial 435 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:00:58,847]\u001b[0m Trial 436 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:01:12,720]\u001b[0m Trial 437 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:01:26,278]\u001b[0m Trial 438 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:01:40,061]\u001b[0m Trial 439 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:01:54,051]\u001b[0m Trial 440 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:02:07,973]\u001b[0m Trial 441 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:02:22,076]\u001b[0m Trial 442 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:02:35,996]\u001b[0m Trial 443 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:02:49,562]\u001b[0m Trial 444 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:03:03,476]\u001b[0m Trial 445 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:03:17,981]\u001b[0m Trial 446 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:03:32,216]\u001b[0m Trial 447 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:03:46,054]\u001b[0m Trial 448 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:03:59,841]\u001b[0m Trial 449 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:04:14,461]\u001b[0m Trial 450 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:04:28,373]\u001b[0m Trial 451 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:04:42,025]\u001b[0m Trial 452 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:04:58,436]\u001b[0m Trial 453 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:05:12,334]\u001b[0m Trial 454 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:05:26,550]\u001b[0m Trial 455 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:05:41,348]\u001b[0m Trial 456 pruned. Trial was pruned at iteration 15.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:05:55,180]\u001b[0m Trial 457 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:06:08,917]\u001b[0m Trial 458 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:06:22,889]\u001b[0m Trial 459 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:06:36,939]\u001b[0m Trial 460 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:06:51,428]\u001b[0m Trial 461 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:07:07,122]\u001b[0m Trial 462 pruned. Trial was pruned at iteration 51.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:07:20,955]\u001b[0m Trial 463 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:07:34,697]\u001b[0m Trial 464 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 16:07:48,739]\u001b[0m Trial 465 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:09:28,512]\u001b[0m Trial 466 finished with value: 0.44491296861127444 and parameters: {'drop_rate': 0.04224130103796482, 'max_drop': 88, 'skip_drop': 0.7344143863771013, 'max_depth': 4, 'min_child_samples': 24, 'bagging_fraction': 0.9247493749126963, 'bagging_freq': 8, 'feature_fraction': 0.6712248512239612, 'feature_fraction_bynode': 0.5085579955816929, 'lambda_l1': 0.053001304830880294, 'lambda_l2': 6.642527388419518e-08, 'learning_rate': 0.09787867917806761, 'num_leaves': 450}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:09:42,436]\u001b[0m Trial 467 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:09:55,965]\u001b[0m Trial 468 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:10:10,172]\u001b[0m Trial 469 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:10:24,112]\u001b[0m Trial 470 pruned. Trial was pruned at iteration 6.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:10:38,852]\u001b[0m Trial 471 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:10:54,216]\u001b[0m Trial 472 pruned. Trial was pruned at iteration 53.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:11:08,089]\u001b[0m Trial 473 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:11:21,537]\u001b[0m Trial 474 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:11:35,819]\u001b[0m Trial 475 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:11:50,134]\u001b[0m Trial 476 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:12:03,940]\u001b[0m Trial 477 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:12:17,699]\u001b[0m Trial 478 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:12:32,007]\u001b[0m Trial 479 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:12:47,091]\u001b[0m Trial 480 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:13:01,428]\u001b[0m Trial 481 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:13:15,310]\u001b[0m Trial 482 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:13:29,122]\u001b[0m Trial 483 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:13:43,016]\u001b[0m Trial 484 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 17:13:56,929]\u001b[0m Trial 485 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 18:11:55,464]\u001b[0m Trial 486 finished with value: 0.4441723438760118 and parameters: {'drop_rate': 0.05467074174077665, 'max_drop': 73, 'skip_drop': 0.736039433902234, 'max_depth': 4, 'min_child_samples': 19, 'bagging_fraction': 0.9467145418274461, 'bagging_freq': 9, 'feature_fraction': 0.7238380108475285, 'feature_fraction_bynode': 0.4084584616552945, 'lambda_l1': 0.2284033820493702, 'lambda_l2': 0.00035585098889373714, 'learning_rate': 0.09044538514299524, 'num_leaves': 303}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-21 19:08:35,214]\u001b[0m Trial 487 finished with value: 0.4451287360001096 and parameters: {'drop_rate': 0.05684083160162887, 'max_drop': 76, 'skip_drop': 0.752408321771864, 'max_depth': 4, 'min_child_samples': 20, 'bagging_fraction': 0.9476733178340816, 'bagging_freq': 9, 'feature_fraction': 0.7578228375046713, 'feature_fraction_bynode': 0.3907093285618804, 'lambda_l1': 0.2971483296763664, 'lambda_l2': 0.00031384649678679253, 'learning_rate': 0.09355569910364583, 'num_leaves': 328}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:08:50,461]\u001b[0m Trial 488 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:12:45,468]\u001b[0m Trial 489 pruned. Trial was pruned at iteration 3431.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:12:59,265]\u001b[0m Trial 490 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:13:12,792]\u001b[0m Trial 491 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:13:27,357]\u001b[0m Trial 492 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:13:41,861]\u001b[0m Trial 493 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:13:57,307]\u001b[0m Trial 494 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:14:11,663]\u001b[0m Trial 495 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 19:24:57,421]\u001b[0m Trial 496 pruned. Trial was pruned at iteration 5574.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:11:14,560]\u001b[0m Trial 497 finished with value: 0.44519838119159816 and parameters: {'drop_rate': 0.06261955200554327, 'max_drop': 74, 'skip_drop': 0.7784301909604692, 'max_depth': 4, 'min_child_samples': 15, 'bagging_fraction': 0.945961585891631, 'bagging_freq': 0, 'feature_fraction': 0.7134303438959362, 'feature_fraction_bynode': 0.4423851961114564, 'lambda_l1': 0.1283030631958206, 'lambda_l2': 9.12512973967472e-05, 'learning_rate': 0.08840424787478662, 'num_leaves': 343}. Best is trial 81 with value: 0.4432953622815359.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:11:27,642]\u001b[0m Trial 498 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:11:39,961]\u001b[0m Trial 499 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:11:52,563]\u001b[0m Trial 500 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:12:05,038]\u001b[0m Trial 501 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:12:17,748]\u001b[0m Trial 502 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:12:30,376]\u001b[0m Trial 503 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:12:43,057]\u001b[0m Trial 504 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:12:55,569]\u001b[0m Trial 505 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:13:08,202]\u001b[0m Trial 506 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:13:20,610]\u001b[0m Trial 507 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:13:33,261]\u001b[0m Trial 508 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:13:45,814]\u001b[0m Trial 509 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:13:58,422]\u001b[0m Trial 510 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:14:10,740]\u001b[0m Trial 511 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:14:23,923]\u001b[0m Trial 512 pruned. Trial was pruned at iteration 13.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:14:36,492]\u001b[0m Trial 513 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:14:49,055]\u001b[0m Trial 514 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:15:01,660]\u001b[0m Trial 515 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:15:14,606]\u001b[0m Trial 516 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:15:27,730]\u001b[0m Trial 517 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:15:40,554]\u001b[0m Trial 518 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:15:52,978]\u001b[0m Trial 519 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:16:05,945]\u001b[0m Trial 520 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:16:19,259]\u001b[0m Trial 521 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:16:32,852]\u001b[0m Trial 522 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:16:46,175]\u001b[0m Trial 523 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:16:59,484]\u001b[0m Trial 524 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:17:12,667]\u001b[0m Trial 525 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:17:25,709]\u001b[0m Trial 526 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-21 20:17:38,838]\u001b[0m Trial 527 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[33m[W 2023-03-21 20:21:37,754]\u001b[0m Trial 528 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<timed exec>\", line 4, in <lambda>\n",
      "  File \"C:\\Users\\Room722\\AppData\\Local\\Temp\\ipykernel_8336\\480918275.py\", line 32, in objective\n",
      "    clf = lgb.train(params,\n",
      "  File \"C:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\lightgbm\\engine.py\", line 292, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"C:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\lightgbm\\basic.py\", line 3021, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X, y)\u001b[0m\n\u001b[0;32m     30\u001b[0m lgb_train \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X\u001b[38;5;241m.\u001b[39miloc[train_idx], y[train_idx])\n\u001b[0;32m     31\u001b[0m lgb_eval \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X\u001b[38;5;241m.\u001b[39miloc[test_idx], y[test_idx], reference\u001b[38;5;241m=\u001b[39mlgb_train)\n\u001b[1;32m---> 32\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlgb_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLightGBMPruningCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary_logloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m preds \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m.\u001b[39miloc[test_idx])\n\u001b[0;32m     40\u001b[0m cv_scores[n] \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mlog_loss(y[test_idx], preds)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO) \n",
    "\n",
    "study = optuna.create_study(direction='minimize')  \n",
    "func = lambda trial: objective(trial, train_df, y_true)\n",
    "study.optimize(func, n_trials=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21b188ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 27min 55s\n",
      "Wall time: 12min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = train_df\n",
    "y = y_true\n",
    "\n",
    "params = {'boosting_type': 'dart',\n",
    "          'drop_rate': 0.04181725177738285,\n",
    "          'max_drop': 90,\n",
    "          'skip_drop': 0.681433217444748,\n",
    "          'objective': 'binary',\n",
    "          'num_iterations': 10000,\n",
    "          'max_depth': 4,\n",
    "          'min_child_samples': 10,\n",
    "          'bagging_fraction': 0.879149476519461,\n",
    "          'bagging_freq': 10,\n",
    "          'feature_fraction': 0.6338351071617777,\n",
    "          'feature_fraction_bynode': 0.4578052622858927,\n",
    "          'lambda_l1': 0.09329820302670715,\n",
    "          'lambda_l2': 0.00038402177706728873,\n",
    "          'learning_rate': 0.08343032559963369,\n",
    "          'num_leaves': 264,\n",
    "          #'device_type': \"gpu\",\n",
    "          'num_threads': 12,\n",
    "          'verbose': -1, # output to stdout info about training process every 200 iterations\n",
    "          'seed': 722\n",
    "         }\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(X, y)\n",
    "clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a510afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_male'] = clf.predict(test_df)\n",
    "test_df[['is_male']].to_csv('v124.1/lgbm_age_single_model10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a25dfabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ceca9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3h 30min 13s\n",
      "Wall time: 18min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = train_df\n",
    "y = y_true\n",
    "\n",
    "params = {'boosting_type': 'dart',\n",
    "          'drop_rate': 0.04181725177738285,\n",
    "          'max_drop': 90,\n",
    "          'skip_drop': 0.681433217444748,\n",
    "          'objective': 'binary',\n",
    "          'num_iterations': 12500,\n",
    "          'max_depth': 4,\n",
    "          'min_child_samples': 10,\n",
    "          'bagging_fraction': 0.879149476519461,\n",
    "          'bagging_freq': 10,\n",
    "          'feature_fraction': 0.6338351071617777,\n",
    "          'feature_fraction_bynode': 0.4578052622858927,\n",
    "          'lambda_l1': 0.09329820302670715,\n",
    "          'lambda_l2': 0.00038402177706728873,\n",
    "          'learning_rate': 0.08343032559963369,\n",
    "          'num_leaves': 264,\n",
    "          #'device_type': \"gpu\",\n",
    "          'num_threads': 12,\n",
    "          'verbose': -1, # output to stdout info about training process every 200 iterations\n",
    "          'seed': 722\n",
    "         }\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(X, y)\n",
    "clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3013cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_df['is_male']\n",
    "test_df['is_male'] = clf.predict(test_df)\n",
    "test_df[['is_male']].to_csv('v124.1/lgbm_age_single_model12500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5c1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da1f6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64ca37ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10h 55min 30s\n",
      "Wall time: 1h 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = train_df\n",
    "y = y_true\n",
    "\n",
    "params = {'boosting_type': 'dart',\n",
    "          'drop_rate': 0.04181725177738285,\n",
    "          'max_drop': 90,\n",
    "          'skip_drop': 0.681433217444748,\n",
    "          'objective': 'binary',\n",
    "          'num_iterations': 10000,\n",
    "          'max_depth': 4,\n",
    "          'min_child_samples': 10,\n",
    "          'bagging_fraction': 0.879149476519461,\n",
    "          'bagging_freq': 10,\n",
    "          'feature_fraction': 0.6338351071617777,\n",
    "          'feature_fraction_bynode': 0.4578052622858927,\n",
    "          'lambda_l1': 0.09329820302670715,\n",
    "          'lambda_l2': 0.00038402177706728873,\n",
    "          'learning_rate': 0.08343032559963369,\n",
    "          'num_leaves': 264,\n",
    "          #'device_type': \"gpu\",\n",
    "          'num_threads': 11,\n",
    "          'verbose': -1, # output to stdout info about training process every 200 iterations\n",
    "          'seed': 722\n",
    "         }\n",
    "\n",
    "preds = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for n, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    lgb_train = lgb.Dataset(X.iloc[train_idx], y[train_idx])\n",
    "    clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                )\n",
    "    preds.append(clf.predict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0edab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_male'] = np.mean(preds, axis=0)\n",
    "test_df[['is_male']].to_csv('v124.1/lgbm_age_5fold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82869ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_env_v1] *",
   "language": "python",
   "name": "conda-env-ml_env_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
