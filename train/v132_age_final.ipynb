{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c01e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix, vstack, load_npz\n",
    "\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sys import getsizeof\n",
    "import gc\n",
    "#from catboost import CatBoostRegressor, cv, Pool, sum_models\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import MaxAbsScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "import vaex\n",
    "import pyarrow.parquet as pq\n",
    "import bisect\n",
    "\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, ProgbarLogger\n",
    "from tensorflow.keras import regularizers as R\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import optimizers as O\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "tf.random.set_seed(722)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ad6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_PATH = 'context_data'\n",
    "SPLIT_SEED = 42\n",
    "DATA_FILE = 'competition_data_final_pqt'\n",
    "TARGET_FILE = 'public_train.pqt'\n",
    "SUBMISSION_FILE = 'submit_2.pqt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e20da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_submit = pq.read_table(f'../{LOCAL_DATA_PATH}/{SUBMISSION_FILE}').to_pandas()\n",
    "tgt = pq.read_table(f'../{LOCAL_DATA_PATH}/{TARGET_FILE}').to_pandas()\n",
    "\n",
    "def age_bucket(x):\n",
    "    return bisect.bisect_left([18,25,35,45,55,65], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028dd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = load_npz('../utils/mat.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35b87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269903, 88072) (144724, 88072)\n"
     ]
    }
   ],
   "source": [
    "idx_tr = tgt['age'][tgt.age > 15].index.values\n",
    "y_train = tgt['age'][tgt.age > 15].map(age_bucket).values.astype(np.int8)\n",
    "y_train[y_train==0] = 1\n",
    "y_train = y_train - 1\n",
    "\n",
    "mat_train = mat[idx_tr]\n",
    "idx_test = id_to_submit.user_id.values\n",
    "mat_test = mat[idx_test]\n",
    "\n",
    "cols_countsum_tr = np.asarray(mat_train.astype(bool).sum(axis=0)).flatten()\n",
    "cols_countsum_test = np.asarray(mat_test.astype(bool).sum(axis=0)).flatten()\n",
    "mask = (cols_countsum_tr > 1) * (cols_countsum_test > 0)\n",
    "\n",
    "mat_train = mat_train[:, mask]\n",
    "mat_test = mat_test[:, mask]\n",
    "print(mat_train.shape, mat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a26fd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>evening</th>\n",
       "      <th>morning</th>\n",
       "      <th>night</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>...</th>\n",
       "      <th>company</th>\n",
       "      <th>model</th>\n",
       "      <th>os</th>\n",
       "      <th>region_name_count</th>\n",
       "      <th>city_name_count</th>\n",
       "      <th>req_max</th>\n",
       "      <th>req_sum</th>\n",
       "      <th>id_rows</th>\n",
       "      <th>days</th>\n",
       "      <th>dates_range</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.554404</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>0.119171</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.134715</td>\n",
       "      <td>0.108808</td>\n",
       "      <td>0.036269</td>\n",
       "      <td>0.440415</td>\n",
       "      <td>...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Galaxy J1 2016 LTE Dual</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>193</td>\n",
       "      <td>131</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.346705</td>\n",
       "      <td>0.295129</td>\n",
       "      <td>0.322827</td>\n",
       "      <td>0.035339</td>\n",
       "      <td>0.127985</td>\n",
       "      <td>0.209169</td>\n",
       "      <td>0.102197</td>\n",
       "      <td>0.098376</td>\n",
       "      <td>0.122254</td>\n",
       "      <td>0.150907</td>\n",
       "      <td>...</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>Mi 9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1047</td>\n",
       "      <td>700</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.481752</td>\n",
       "      <td>0.316302</td>\n",
       "      <td>0.187348</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.153285</td>\n",
       "      <td>0.128954</td>\n",
       "      <td>0.148418</td>\n",
       "      <td>0.150852</td>\n",
       "      <td>0.104623</td>\n",
       "      <td>0.128954</td>\n",
       "      <td>...</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>Honor 9 Lite</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>411</td>\n",
       "      <td>356</td>\n",
       "      <td>50</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352727</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.178182</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.185455</td>\n",
       "      <td>0.065455</td>\n",
       "      <td>0.116364</td>\n",
       "      <td>0.123636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>Huawei Device Company Limited</td>\n",
       "      <td>P Smart 2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>275</td>\n",
       "      <td>188</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.348777</td>\n",
       "      <td>0.265122</td>\n",
       "      <td>0.371943</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>0.212355</td>\n",
       "      <td>0.164736</td>\n",
       "      <td>0.185328</td>\n",
       "      <td>0.141570</td>\n",
       "      <td>0.118404</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>...</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>Nova 3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>777</td>\n",
       "      <td>591</td>\n",
       "      <td>20</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              day   evening   morning     night    Friday    Monday  Saturday  \\\n",
       "user_id                                                                         \n",
       "0        0.554404  0.321244  0.119171  0.005181  0.056995  0.020725  0.134715   \n",
       "1        0.346705  0.295129  0.322827  0.035339  0.127985  0.209169  0.102197   \n",
       "2        0.481752  0.316302  0.187348  0.014599  0.153285  0.128954  0.148418   \n",
       "3        0.352727  0.454545  0.178182  0.014545  0.240000  0.185455  0.065455   \n",
       "4        0.348777  0.265122  0.371943  0.014157  0.212355  0.164736  0.185328   \n",
       "\n",
       "           Sunday  Thursday   Tuesday  ...                        company  \\\n",
       "user_id                                ...                                  \n",
       "0        0.108808  0.036269  0.440415  ...                        Samsung   \n",
       "1        0.098376  0.122254  0.150907  ...                         Xiaomi   \n",
       "2        0.150852  0.104623  0.128954  ...                         Huawei   \n",
       "3        0.116364  0.123636  0.090909  ...  Huawei Device Company Limited   \n",
       "4        0.141570  0.118404  0.072072  ...                         Huawei   \n",
       "\n",
       "                           model os region_name_count city_name_count req_max  \\\n",
       "user_id                                                                         \n",
       "0        Galaxy J1 2016 LTE Dual  1                 1               1       5   \n",
       "1                           Mi 9  1                 3               6       6   \n",
       "2                   Honor 9 Lite  1                 1               1       4   \n",
       "3                   P Smart 2021  1                 1               1       5   \n",
       "4                         Nova 3  1                 5               9       5   \n",
       "\n",
       "         req_sum  id_rows  days  dates_range  \n",
       "user_id                                       \n",
       "0            193      131    17           18  \n",
       "1           1047      700    19           20  \n",
       "2            411      356    50           57  \n",
       "3            275      188    15           16  \n",
       "4            777      591    20           42  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df = pd.read_csv('../utils/feat_gen_df3.csv', index_col='user_id')\n",
    "feat_df['os'] = feat_df['os'].map({'iOS': 0, 'Android': 1})\n",
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959d7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feat = feat_df.drop(['region_name', 'city_name', 'company', 'model'], axis=1).values\n",
    "cont_feat_train = cont_feat[idx_tr]\n",
    "cont_feat_test = cont_feat[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5f5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = feat_df[['region_name', 'city_name', 'company', 'model']]\n",
    "cat_feat = np.stack([cat_df[col].astype('category').cat.codes.values for col in cat_df]).T\n",
    "cat_feat_train = cat_feat[idx_tr]\n",
    "cat_feat_test = cat_feat[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805eaa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269903, 173452) (144724, 173452)\n"
     ]
    }
   ],
   "source": [
    "mat_pod = load_npz('../utils/mat_pod.npz')\n",
    "mat_pod_train = mat_pod[idx_tr]\n",
    "mat_pod_test = mat_pod[idx_test]\n",
    "\n",
    "mat_pod_cols_countsum_tr = np.asarray(mat_pod_train.astype(bool).sum(axis=0)).flatten()\n",
    "mat_pod_cols_countsum_test = np.asarray(mat_pod_test.astype(bool).sum(axis=0)).flatten()\n",
    "mat_pod_mask = (mat_pod_cols_countsum_tr > 1) * (mat_pod_cols_countsum_test > 0)\n",
    "\n",
    "mat_pod_train = mat_pod_train[:, mat_pod_mask]\n",
    "mat_pod_test = mat_pod_test[:, mat_pod_mask]\n",
    "print(mat_pod_train.shape, mat_pod_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b9b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x1_vals, x2_vals, x3_vals, x4_vals, y_vals, batch_size, split_idx, shuffle_idx=False):\n",
    "        self.x1_vals = x1_vals\n",
    "        self.x2_vals = x2_vals\n",
    "        self.x3_vals = x3_vals\n",
    "        self.x4_vals = x4_vals\n",
    "        self.y_vals = y_vals\n",
    "        self.inds = split_idx\n",
    "        self.shuffle_idx = shuffle_idx\n",
    "        if shuffle_idx:\n",
    "            shuffle(self.inds)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        from_ind = self.batch_size * item\n",
    "        to_ind = self.batch_size * (item + 1)\n",
    "        batch_x1 = self.x1_vals[np.sort(self.inds[from_ind:to_ind])].todense()\n",
    "        batch_x2 = self.x2_vals[np.sort(self.inds[from_ind:to_ind])]\n",
    "        batch_x3 = self.x3_vals[np.sort(self.inds[from_ind:to_ind])]\n",
    "        batch_x4 = self.x4_vals[np.sort(self.inds[from_ind:to_ind])].todense()\n",
    "        batch_y = self.y_vals[np.sort(self.inds[from_ind:to_ind])]\n",
    "        return ([batch_x1, batch_x2, batch_x3, batch_x4], tf.one_hot(batch_y, depth=6))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle_idx:\n",
    "            shuffle(self.inds)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.inds) / float(self.batch_size)))\n",
    "    \n",
    "    \n",
    "class DataGenerator_test(Sequence):\n",
    "    def __init__(self, x1_vals, x2_vals, x3_vals, x4_vals, batch_size, split_idx, shuffle_idx=False):\n",
    "        self.x1_vals = x1_vals\n",
    "        self.x2_vals = x2_vals\n",
    "        self.x3_vals = x3_vals\n",
    "        self.x4_vals = x4_vals\n",
    "        self.inds = split_idx\n",
    "        self.shuffle_idx = shuffle_idx\n",
    "        if shuffle_idx:\n",
    "            shuffle(self.inds)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        from_ind = self.batch_size * item\n",
    "        to_ind = self.batch_size * (item + 1)\n",
    "        batch_x1 = self.x1_vals[np.sort(self.inds[from_ind:to_ind])].todense()\n",
    "        batch_x2 = self.x2_vals[np.sort(self.inds[from_ind:to_ind])]\n",
    "        batch_x3 = self.x3_vals[np.sort(self.inds[from_ind:to_ind])]\n",
    "        batch_x4 = self.x4_vals[np.sort(self.inds[from_ind:to_ind])].todense()\n",
    "        return ([batch_x1, batch_x2, batch_x3, batch_x4],)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle_idx:\n",
    "            shuffle(self.inds)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.inds) / float(self.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f8c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(L.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.linear = L.Dense(units)\n",
    "        self.sigmoid = L.Dense(units, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs) * self.sigmoid(inputs)\n",
    "    \n",
    "    \n",
    "class GatedResidualNetwork(L.Layer):\n",
    "    def __init__(self, units, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.relu_dense = L.Dense(units, activation=\"relu\")\n",
    "        self.linear_dense = L.Dense(units)\n",
    "        self.dropout = L.Dropout(dropout_rate)\n",
    "        self.gated_linear_unit = GatedLinearUnit(units)\n",
    "        self.layer_norm = L.LayerNormalization()\n",
    "        self.project = L.Dense(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.relu_dense(inputs)\n",
    "        x = self.linear_dense(x)\n",
    "        x = self.dropout(x)\n",
    "        if inputs.shape[-1] != self.units:\n",
    "            inputs = self.project(inputs)\n",
    "        x = inputs + self.gated_linear_unit(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VariableSelection(L.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.grns = list()\n",
    "        # Create a GRN for each feature independently\n",
    "        for idx in range(num_features):\n",
    "            grn = GatedResidualNetwork(units, dropout_rate)\n",
    "            self.grns.append(grn)\n",
    "        # Create a GRN for the concatenation of all the features\n",
    "        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n",
    "        self.softmax = L.Dense(units=num_features, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        v = L.concatenate(inputs)\n",
    "        v = self.grn_concat(v)\n",
    "        v = tf.expand_dims(self.softmax(v), axis=-1)\n",
    "\n",
    "        x = []\n",
    "        for idx, input_ in enumerate(inputs):\n",
    "            x.append(self.grns[idx](input_))\n",
    "        x = tf.stack(x, axis=1)\n",
    "\n",
    "        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class VariableSelectionFlow(L.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate, dense_units=None):\n",
    "        super().__init__()\n",
    "        self.variableselection = VariableSelection(num_features, units, dropout_rate)\n",
    "        self.split = L.Lambda(lambda t: tf.split(t, num_features, axis=-1))\n",
    "        self.dense = dense_units\n",
    "        if dense_units:\n",
    "            self.dense_list = [L.Dense(dense_units, \\\n",
    "                                       activation='linear') \\\n",
    "                               for _ in tf.range(num_features)\n",
    "                              ]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        split_input = self.split(inputs)\n",
    "        if self.dense:\n",
    "            l = [self.dense_list[i](split_input[i]) for i in range(len(self.dense_list))]\n",
    "        else:\n",
    "            l = split_input\n",
    "        return self.variableselection(l)        \n",
    "    \n",
    "    \n",
    "def smish(x):\n",
    "    return x * K.tanh(K.log(1 + K.sigmoid(x)))\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(L.Dense(units, activation=activation))\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(L.Dropout(dropout_rate))\n",
    "\n",
    "    return tf.keras.Sequential(mlp_layers, name=name)\n",
    "\n",
    "\n",
    "class TransformerBlock(L.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.15, num_transformer_blocks=3):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.att = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)\n",
    "        self.ffn = create_mlp(\n",
    "            hidden_units=ff_dim,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=tf.keras.activations.gelu,\n",
    "            normalization_layer=L.LayerNormalization(epsilon=1e-6),\n",
    "        )\n",
    "        self.layernorm1 = L.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = L.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        for block_idx in range(num_transformer_blocks):\n",
    "            attn_output = self.att(inputs, inputs)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            ffn_output = self.ffn(out1)\n",
    "            inputs = self.layernorm2(out1 + ffn_output)\n",
    "        return inputs\n",
    "\n",
    "    \n",
    "class Wt_Add(L.Layer):\n",
    "    def __init__(self, units=1, input_dim=1):\n",
    "        super(Wt_Add, self).__init__()\n",
    "        w_init = tf.random_normal_initializer(mean=1.0)\n",
    "        self.w1 = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.w2 = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )        \n",
    "        \n",
    "    def call(self, input1, input2):\n",
    "        return tf.multiply(input1,self.w1) + tf.multiply(input2, self.w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e05b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "units_1 = 256\n",
    "units_2 = 64\n",
    "units_22 = 128\n",
    "dropout_1 = 0.1\n",
    "dropout_2 = 0.1\n",
    "dropout_22 = 0.1\n",
    "\n",
    "\n",
    "INIT_LR = 1e-5\n",
    "MAX_LR = 1e-3\n",
    "steps_per_epoch = 1055\n",
    "\n",
    "\n",
    "dropout_rate = 0.10\n",
    "num_transformer_blocks = 3  # Number of transformer blocks.\n",
    "num_heads = 4  # Number of attention heads.\n",
    "embedding_dims = 32  # Embedding dimensions of the categorical features.\n",
    "vocab_len = [80, 950, 37, 599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b16742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbf306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###__--__###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8ed2695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______fold 1______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3837s 3s/step - loss: 1.2966 - f1_score: 0.4282\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3048s 3s/step - loss: 1.1764 - f1_score: 0.4894\n",
      "566/566 [==============================] - 627s 958ms/step\n",
      "______fold 2______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3942s 3s/step - loss: 1.2949 - f1_score: 0.4277\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3115s 3s/step - loss: 1.1774 - f1_score: 0.4886\n",
      "566/566 [==============================] - 656s 991ms/step\n",
      "______fold 3______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3923s 3s/step - loss: 1.2954 - f1_score: 0.4264\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3096s 3s/step - loss: 1.1804 - f1_score: 0.4875\n",
      "566/566 [==============================] - 660s 991ms/step\n",
      "______fold 4______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3940s 3s/step - loss: 1.3012 - f1_score: 0.4244\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3105s 3s/step - loss: 1.1782 - f1_score: 0.4885\n",
      "566/566 [==============================] - 637s 954ms/step\n",
      "______fold 5______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3921s 3s/step - loss: 1.2888 - f1_score: 0.4329\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3111s 3s/step - loss: 1.1761 - f1_score: 0.4900\n",
      "566/566 [==============================] - 618s 924ms/step\n",
      "______fold 6______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3937s 3s/step - loss: 1.2881 - f1_score: 0.4313\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3128s 3s/step - loss: 1.1767 - f1_score: 0.4889\n",
      "566/566 [==============================] - 652s 982ms/step\n",
      "______fold 7______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3921s 3s/step - loss: 1.2908 - f1_score: 0.4323\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3103s 3s/step - loss: 1.1802 - f1_score: 0.4873\n",
      "566/566 [==============================] - 659s 995ms/step\n",
      "______fold 8______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3944s 3s/step - loss: 1.2876 - f1_score: 0.4327\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3131s 3s/step - loss: 1.1750 - f1_score: 0.4913\n",
      "566/566 [==============================] - 660s 997ms/step\n",
      "______fold 9______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3950s 3s/step - loss: 1.2901 - f1_score: 0.4312\n",
      "Epoch 2/2\n",
      " 495/1055 [=============>................] - ETA: 27:20 - loss: 1.1902 - f1_score: 0.4823"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nOOM when allocating tensor with shape[256,16,1,44016] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d/mul/Mul_1-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_8411640]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:94\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\ml_env_v1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nOOM when allocating tensor with shape[256,16,1,44016] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model/conv2d/mul/Mul_1-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_8411640]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#policy_m16 = mixed_precision.Policy('mixed_float16')\n",
    "#policy_32 = mixed_precision.Policy('float32')\n",
    "\n",
    "test_gen = DataGenerator_test(mat_test,\\\n",
    "                              cont_feat_test,\\\n",
    "                              cat_feat_test,\\\n",
    "                              mat_pod_test,\\\n",
    "                              batch_size,\n",
    "                              np.arange(mat_test.shape[0])\n",
    "                           )\n",
    "\n",
    "for n in range(10):\n",
    "    print(f'______fold {n+1}______')\n",
    "    train_gen = DataGenerator(mat_train,\\\n",
    "                              cont_feat_train,\\\n",
    "                              cat_feat_train,\\\n",
    "                              mat_pod_train,\\\n",
    "                              y_train,\\\n",
    "                              batch_size,\\\n",
    "                              np.arange(mat_train.shape[0]),\\\n",
    "                              shuffle_idx=True\n",
    "                             )\n",
    "    \n",
    "    #mixed_precision.set_global_policy(policy_m16)\n",
    "\n",
    "    inputs_1 = tf.keras.Input(shape=(88072,))\n",
    "    r1_1 = L.Reshape((1,88072,1))(inputs_1)\n",
    "    cnn_1 = L.Conv2D(16, (1,41), strides=2, activation=smish)(r1_1)\n",
    "    \n",
    "    #mixed_precision.set_global_policy(policy_32)\n",
    "\n",
    "    d_1 = L.Dense(1, activation=smish)(cnn_1)\n",
    "    r2_1 = L.Reshape((44016,))(d_1)\n",
    "    features_1 = VariableSelectionFlow(336, units_1, dropout_1)(r2_1)\n",
    "    \n",
    "   \n",
    "    inputs_2 = tf.keras.Input(shape=(20,), dtype=tf.int32)\n",
    "    features_2 = VariableSelectionFlow(20, units_2, dropout_2, dense_units=1)(inputs_2)\n",
    "\n",
    "    \n",
    "    inputs_3 = tf.keras.Input(shape=(4,), dtype=tf.int16)\n",
    "    n_0 = L.Lambda(lambda t: tf.split(t, 4, axis=-1))(inputs_3)\n",
    "    emb = [L.Embedding(input_dim=vocab_len[n], output_dim=embedding_dims)(l) for n, l in enumerate(n_0)]\n",
    "    cat_emb = tf.concat(emb, axis=1)    \n",
    "    transf_cat = TransformerBlock(embed_dim=embedding_dims, \\\n",
    "                                 num_heads=num_heads, \\\n",
    "                                 ff_dim=[embedding_dims], \\\n",
    "                                 dropout_rate=dropout_rate, \\\n",
    "                                 num_transformer_blocks=num_transformer_blocks\n",
    "                                )(cat_emb)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    cat_features = L.Flatten()(transf_cat)\n",
    "\n",
    "    #mixed_precision.set_global_policy(policy_m16)    \n",
    "    \n",
    "    inputs_4 = tf.keras.Input(shape=(173452,))\n",
    "    r1_4 = L.Reshape((1,173452,1))(inputs_4)\n",
    "    cnn_4 = L.Conv2D(6, (1,53), strides=2, activation=smish)(r1_4)\n",
    "    \n",
    "    #mixed_precision.set_global_policy(policy_32)\n",
    "    \n",
    "    d_4 = L.Dense(1, activation=smish)(cnn_4)    \n",
    "    r2_4 = L.Reshape((86700,))(d_4)\n",
    "    features_4 = VariableSelectionFlow(425, units_1, dropout_1)(r2_4)   \n",
    "\n",
    "    \n",
    "    add_1_4 = Wt_Add(units=units_1)(features_1, features_4)\n",
    "    \n",
    "    \n",
    "    concat1 = L.Concatenate()([features_2, cat_features, add_1_4])\n",
    "    \n",
    "    features_22 = VariableSelectionFlow(concat1.shape[-1], units_22, dropout_22)(concat1)\n",
    "    \n",
    "    dense_out = L.Dense(6)(features_22)\n",
    "    outputs = L.Activation(\"softmax\", dtype='float32')(dense_out)\n",
    "\n",
    "    model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4], outputs=outputs)\n",
    "                \n",
    "    clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
    "        maximal_learning_rate=MAX_LR,\n",
    "        scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "        step_size=1 * steps_per_epoch\n",
    "        )\n",
    "    \n",
    "    opt = O.Adam(learning_rate=clr, epsilon=1e-09)\n",
    "    loss = categorical_crossentropy\n",
    "\n",
    "    model.compile(optimizer=opt, \n",
    "                    loss=loss,\n",
    "                    metrics=[F1Score(num_classes=6, average='weighted')]\n",
    "                 )\n",
    "    \n",
    "    model.fit(train_gen,\n",
    "                epochs=2\n",
    "            )\n",
    "\n",
    "    y_test_df = pd.DataFrame(idx_test).rename({0: 'user_id'}, axis=1)\n",
    "    y_test_df[[str(i) for i in (range(1,7))]] = model.predict(test_gen)\n",
    "    y_test_df = y_test_df.set_index('user_id', drop=True)\n",
    "    y_test_df.to_csv(f'v132/fold_{n+1}/y_test.csv')\n",
    "    \n",
    "    del model, clr, opt, loss\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35cedecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______fold 9______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3822s 3s/step - loss: 1.2982 - f1_score: 0.4257\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3064s 3s/step - loss: 1.1777 - f1_score: 0.4894\n",
      "566/566 [==============================] - 622s 933ms/step\n",
      "______fold 10______\n",
      "Epoch 1/2\n",
      "1055/1055 [==============================] - 3994s 3s/step - loss: 1.2946 - f1_score: 0.4287\n",
      "Epoch 2/2\n",
      "1055/1055 [==============================] - 3136s 3s/step - loss: 1.1779 - f1_score: 0.4887\n",
      "566/566 [==============================] - 647s 960ms/step\n",
      "CPU times: total: 2h 41min 9s\n",
      "Wall time: 4h 17min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#policy_m16 = mixed_precision.Policy('mixed_float16')\n",
    "#policy_32 = mixed_precision.Policy('float32')\n",
    "\n",
    "test_gen = DataGenerator_test(mat_test,\\\n",
    "                              cont_feat_test,\\\n",
    "                              cat_feat_test,\\\n",
    "                              mat_pod_test,\\\n",
    "                              batch_size,\n",
    "                              np.arange(mat_test.shape[0])\n",
    "                           )\n",
    "\n",
    "for n in range(8,10):\n",
    "    print(f'______fold {n+1}______')\n",
    "    train_gen = DataGenerator(mat_train,\\\n",
    "                              cont_feat_train,\\\n",
    "                              cat_feat_train,\\\n",
    "                              mat_pod_train,\\\n",
    "                              y_train,\\\n",
    "                              batch_size,\\\n",
    "                              np.arange(mat_train.shape[0]),\\\n",
    "                              shuffle_idx=True\n",
    "                             )\n",
    "    \n",
    "    #mixed_precision.set_global_policy(policy_m16)\n",
    "\n",
    "    inputs_1 = tf.keras.Input(shape=(88072,))\n",
    "    r1_1 = L.Reshape((1,88072,1))(inputs_1)\n",
    "    cnn_1 = L.Conv2D(16, (1,41), strides=2, activation=smish)(r1_1)\n",
    "    \n",
    "    #mixed_precision.set_global_policy(policy_32)\n",
    "\n",
    "    d_1 = L.Dense(1, activation=smish)(cnn_1)\n",
    "    r2_1 = L.Reshape((44016,))(d_1)\n",
    "    features_1 = VariableSelectionFlow(336, units_1, dropout_1)(r2_1)\n",
    "    \n",
    "   \n",
    "    inputs_2 = tf.keras.Input(shape=(20,), dtype=tf.int32)\n",
    "    features_2 = VariableSelectionFlow(20, units_2, dropout_2, dense_units=1)(inputs_2)\n",
    "\n",
    "    \n",
    "    inputs_3 = tf.keras.Input(shape=(4,), dtype=tf.int16)\n",
    "    n_0 = L.Lambda(lambda t: tf.split(t, 4, axis=-1))(inputs_3)\n",
    "    emb = [L.Embedding(input_dim=vocab_len[n], output_dim=embedding_dims)(l) for n, l in enumerate(n_0)]\n",
    "    cat_emb = tf.concat(emb, axis=1)    \n",
    "    transf_cat = TransformerBlock(embed_dim=embedding_dims, \\\n",
    "                                 num_heads=num_heads, \\\n",
    "                                 ff_dim=[embedding_dims], \\\n",
    "                                 dropout_rate=dropout_rate, \\\n",
    "                                 num_transformer_blocks=num_transformer_blocks\n",
    "                                )(cat_emb)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    cat_features = L.Flatten()(transf_cat)\n",
    "\n",
    "    #mixed_precision.set_global_policy(policy_m16)    \n",
    "    \n",
    "    inputs_4 = tf.keras.Input(shape=(173452,))\n",
    "    r1_4 = L.Reshape((1,173452,1))(inputs_4)\n",
    "    cnn_4 = L.Conv2D(6, (1,53), strides=2, activation=smish)(r1_4)\n",
    "    \n",
    "    #mixed_precision.set_global_policy(policy_32)\n",
    "    \n",
    "    d_4 = L.Dense(1, activation=smish)(cnn_4)    \n",
    "    r2_4 = L.Reshape((86700,))(d_4)\n",
    "    features_4 = VariableSelectionFlow(425, units_1, dropout_1)(r2_4)   \n",
    "\n",
    "    \n",
    "    add_1_4 = Wt_Add(units=units_1)(features_1, features_4)\n",
    "    \n",
    "    \n",
    "    concat1 = L.Concatenate()([features_2, cat_features, add_1_4])\n",
    "    \n",
    "    features_22 = VariableSelectionFlow(concat1.shape[-1], units_22, dropout_22)(concat1)\n",
    "    \n",
    "    dense_out = L.Dense(6)(features_22)\n",
    "    outputs = L.Activation(\"softmax\", dtype='float32')(dense_out)\n",
    "\n",
    "    model = Model(inputs=[inputs_1, inputs_2, inputs_3, inputs_4], outputs=outputs)\n",
    "                \n",
    "    clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
    "        maximal_learning_rate=MAX_LR,\n",
    "        scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "        step_size=1 * steps_per_epoch\n",
    "        )\n",
    "    \n",
    "    opt = O.Adam(learning_rate=clr, epsilon=1e-09)\n",
    "    loss = categorical_crossentropy\n",
    "\n",
    "    model.compile(optimizer=opt, \n",
    "                    loss=loss,\n",
    "                    metrics=[F1Score(num_classes=6, average='weighted')]\n",
    "                 )\n",
    "    \n",
    "    model.fit(train_gen,\n",
    "                epochs=2\n",
    "            )\n",
    "\n",
    "    y_test_df = pd.DataFrame(idx_test).rename({0: 'user_id'}, axis=1)\n",
    "    y_test_df[[str(i) for i in (range(1,7))]] = model.predict(test_gen)\n",
    "    y_test_df = y_test_df.set_index('user_id', drop=True)\n",
    "    y_test_df.to_csv(f'v132/fold_{n+1}/y_test.csv')\n",
    "    \n",
    "    del model, clr, opt, loss\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241c910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_env_v1] *",
   "language": "python",
   "name": "conda-env-ml_env_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
